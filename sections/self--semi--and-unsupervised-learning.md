# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/transfer-low-shot-and-continual-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/self--semi--meta--unsupervised-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Self-, Semi-, and Unsupervised Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-12-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-11-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-9-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-1-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV | [![GitHub](https://img.shields.io/github/stars/jspenmar/slowtv_monodepth?style=flat)](https://github.com/jspenmar/slowtv_monodepth) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Spencer_Kick_Back__Relax_Learning_to_Reconstruct_the_World_by_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10713-b31b1b.svg)](https://arxiv.org/abs/2307.10713) | :heavy_minus_sign: |
| Novel Scenes & Classes: Towards Adaptive Open-Set Object Detection | [![GitHub](https://img.shields.io/github/stars/CityU-AIM-Group/SOMA?style=flat)](https://github.com/CityU-AIM-Group/SOMA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Novel_Scenes__Classes_Towards_Adaptive_Open-set_Object_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Improving Unsupervised Visual Program Inference with Code Rewriting Families | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bardofcodes.github.io/coref/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ganeshan_Improving_Unsupervised_Visual_Program_Inference_with_Code_Rewriting_Families_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14972-b31b1b.svg)](https://arxiv.org/abs/2309.14972) | :heavy_minus_sign: |
| Denoising Diffusion Autoencoders are Unified Self-Supervised Learners | [![GitHub](https://img.shields.io/github/stars/FutureXiang/ddae?style=flat)](https://github.com/FutureXiang/ddae) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_Denoising_Diffusion_Autoencoders_are_Unified_Self-supervised_Learners_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09769-b31b1b.svg)](https://arxiv.org/abs/2303.09769) | :heavy_minus_sign: |
| Self-Ordering Point Clouds | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Self-Ordering_Point_Clouds_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00961-b31b1b.svg)](https://arxiv.org/abs/2304.00961) | :heavy_minus_sign: |
| MOST: Multiple Object Localization with Self-Supervised Transformers for Object Discovery | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rssaketh.github.io/most) <br /> [![GitHub](https://img.shields.io/github/stars/rssaketh/MOST?style=flat)](https://github.com/rssaketh/MOST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Rambhatla_MOST_Multiple_Object_Localization_with_Self-Supervised_Transformers_for_Object_Discovery_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05387-b31b1b.svg)](https://arxiv.org/abs/2304.05387) | :heavy_minus_sign: |
| CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jellyheadandrew.github.io/projects/chorus/) <br /> [![GitHub](https://img.shields.io/github/stars/jellyheadandrew/CHORUS?style=flat)](https://github.com/jellyheadandrew/CHORUS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_CHORUS__Learning_Canonicalized_3D_Human-Object_Spatial_Relations_from_Unbounded_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12288-b31b1b.svg)](https://arxiv.org/abs/2308.12288) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3WwUdKsbqKQ) |
| Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/dcp15/ISR_ICCV2023_Oral?style=flat)](https://github.com/dcp15/ISR_ICCV2023_Oral) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dou_Identity-Seeking_Self-Supervised_Representation_Learning_for_Generalizable_Person_Re-Identification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08887-b31b1b.svg)](https://arxiv.org/abs/2308.08887) | :heavy_minus_sign: |
| Anatomical Invariance Modeling and Semantic Alignment for Self-Supervised Learning in 3D Medical Image Analysis | [![GitHub](https://img.shields.io/github/stars/alibaba-damo-academy/alice?style=flat)](https://github.com/alibaba-damo-academy/alice) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Anatomical_Invariance_Modeling_and_Semantic_Alignment_for_Self-supervised_Learning_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.05615-b31b1b.svg)](https://arxiv.org/abs/2302.05615) | :heavy_minus_sign: |
| IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization | [![GitHub](https://img.shields.io/github/stars/nukezil/IOMatch?style=flat)](https://github.com/nukezil/IOMatch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_IOMatch_Simplifying_Open-Set_Semi-Supervised_Learning_with_Joint_Inliers_and_Outliers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13168-b31b1b.svg)](https://arxiv.org/abs/2308.13168) | :heavy_minus_sign: |
| Enhancing Sample Utilization through Sample Adaptive Augmentation in Semi-Supervised Learning | [![GitHub](https://img.shields.io/github/stars/GuanGui-nju/SAA?style=flat)](https://github.com/GuanGui-nju/SAA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gui_Enhancing_Sample_Utilization_through_Sample_Adaptive_Augmentation_in_Semi-Supervised_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.03598-b31b1b.svg)](https://arxiv.org/abs/2309.03598) | :heavy_minus_sign: |
| When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_When_Noisy_Labels_Meet_Long_Tail_Dilemmas_A_Representation_Calibration_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10955-b31b1b.svg)](https://arxiv.org/abs/2211.10955) | :heavy_minus_sign: |
