# ICCV-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/neural-generative-models.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-graphics-and-robotics.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
    </a>
</div>

## Vision and Language

![Section Papers](https://img.shields.io/badge/Section%20Papers-115-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-97-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-70-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-9-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11446-b31b1b.svg)](https://arxiv.org/abs/2211.11446) | :heavy_minus_sign: |
| DiffusionRet: Generative Text-Video Retrieval with Diffusion Model | [![GitHub](https://img.shields.io/github/stars/jpthu17/DiffusionRet)](https://github.com/jpthu17/DiffusionRet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jin_DiffusionRet_Generative_Text-Video_Retrieval_with_Diffusion_Model_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09867-b31b1b.svg)](https://arxiv.org/abs/2303.09867) | :heavy_minus_sign: |
| Explore and Tell: Embodied Visual Captioning in 3D Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aim3-ruc.github.io/ExploreAndTell/) <br /> [![GitHub](https://img.shields.io/github/stars/HAWLYQ/ET-Cap)](https://github.com/HAWLYQ/ET-Cap) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Explore_and_Tell_Embodied_Visual_Captioning_in_3D_Environments_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10447-b31b1b.svg)](https://arxiv.org/abs/2308.10447) | :heavy_minus_sign: |
| Distilling Large Vision-Language Model with Out-of-Distribution Generalizability | [![GitHub](https://img.shields.io/github/stars/xuanlinli17/large_vlm_distillation_ood)](https://github.com/xuanlinli17/large_vlm_distillation_ood) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Distilling_Large_Vision-Language_Model_with_Out-of-Distribution_Generalizability_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03135-b31b1b.svg)](https://arxiv.org/abs/2307.03135) | :heavy_minus_sign: |
| Learning Trajectory-Word Alignments for Video-Language Tasks | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Learning_Trajectory-Word_Alignments_for_Video-Language_Tasks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01953-b31b1b.svg)](https://arxiv.org/abs/2301.01953) | :heavy_minus_sign: |
| Variational Causal Inference Network for Explanatory Visual Question Answering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xue_Variational_Causal_Inference_Network_for_Explanatory_Visual_Question_Answering_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TextManiA: Enriching Visual Feature by Text-Driven Manifold Augmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://textmania.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/postech-ami/TextManiA)](https://github.com/postech-ami/TextManiA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye-Bin_TextManiA_Enriching_Visual_Feature_by_Text-driven_Manifold_Augmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14611-b31b1b.svg)](https://arxiv.org/abs/2307.14611) | :heavy_minus_sign: |
| Segment Every Reference Object in Spatial and Temporal Spaces | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Segment_Every_Reference_Object_in_Spatial_and_Temporal_Spaces_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06571-b31b1b.svg)](https://arxiv.org/abs/2303.06571) | :heavy_minus_sign: |
| Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Misalign_Contrast_then_Distill_Rethinking_Misalignments_in_Language-Image_Pre-training_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Toward Multi-Granularity Decision-Making: Explicit Visual Reasoning with Hierarchical Knowledge | [![GitHub](https://img.shields.io/github/stars/SuperJohnZhang/HCNMN)](https://github.com/SuperJohnZhang/HCNMN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Toward_Multi-Granularity_Decision-Making_Explicit_Visual_Reasoning_with_Hierarchical_Knowledge_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Moment Detection in Long Tutorial Videos | [![GitHub](https://img.shields.io/github/stars/ioanacroi/longmoment-detr)](https://github.com/ioanacroi/longmoment-detr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Croitoru_Moment_Detection_in_Long_Tutorial_Videos_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Not All Features Matter: Enhancing Few-Shot CLIP with Adaptive Prior Refinement | [![GitHub](https://img.shields.io/github/stars/yangyangyang127/APE)](https://github.com/yangyangyang127/APE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01195-b31b1b.svg)](https://arxiv.org/abs/2304.01195) | :heavy_minus_sign: |
| Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://whoops-benchmark.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bitton-Guetta_Breaking_Common_Sense_WHOOPS_A_Vision-and-Language_Benchmark_of_Synthetic_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07274-b31b1b.svg)](https://arxiv.org/abs/2303.07274) | :heavy_minus_sign: |
| Advancing Referring Expression Segmentation Beyond Single Image | [![GitHub](https://img.shields.io/github/stars/shikras/d-cube)](https://github.com/shikras/d-cube) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Advancing_Referring_Expression_Segmentation_Beyond_Single_Image_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.12452-b31b1b.svg)](https://arxiv.org/abs/2305.12452) | :heavy_minus_sign: |
| PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-World Learning | [![GitHub](https://img.shields.io/github/stars/yangyangyang127/PointCLIP_V2)](https://github.com/yangyangyang127/PointCLIP_V2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11682-b31b1b.svg)](https://arxiv.org/abs/2211.11682) | :heavy_minus_sign: |
| Unsupervised Prompt Tuning for Text-Driven Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Unsupervised_Prompt_Tuning_for_Text-Driven_Object_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Distilling_Coarse-to-Fine_Semantic_Matching_Knowledge_for_Weakly_Supervised_3D_Visual_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09267-b31b1b.svg)](https://arxiv.org/abs/2307.09267) | :heavy_minus_sign: |
| I can't Believe there's no Images! Learning Visual Tasks using Only Language Supervision | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://prior.allenai.org/projects/close) <br /> [![GitHub](https://img.shields.io/github/stars/allenai/close)](https://github.com/allenai/close) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gu_I_Cant_Believe_Theres_No_Images_Learning_Visual_Tasks_Using_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09778-b31b1b.svg)](https://arxiv.org/abs/2211.09778) | :heavy_minus_sign: |
| Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples | [![GitHub](https://img.shields.io/github/stars/hengliusky/Few_shot_RVOS)](https://github.com/hengliusky/Few_shot_RVOS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Learning_Cross-Modal_Affinity_for_Referring_Video_Object_Segmentation_Targeting_Limited_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02041-b31b1b.svg)](https://arxiv.org/abs/2309.02041) | :heavy_minus_sign: |
| MeViS: A Large-Scale Benchmark for Video Segmentation with Motion Expressions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://henghuiding.github.io/MeViS/) <br /> [![GitHub](https://img.shields.io/github/stars/henghuiding/MeViS)](https://github.com/henghuiding/MeViS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_MeViS_A_Large-scale_Benchmark_for_Video_Segmentation_with_Motion_Expressions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08544-b31b1b.svg)](https://arxiv.org/abs/2308.08544) | :heavy_minus_sign: |
| Diverse Data Augmentation with Diffusions for Effective Test-Time Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/chunmeifeng/DiffTPT)](https://github.com/chunmeifeng/DiffTPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Diverse_Data_Augmentation_with_Diffusions_for_Effective_Test-time_Prompt_Tuning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06038-b31b1b.svg)](https://arxiv.org/abs/2308.06038) | :heavy_minus_sign: |
| ShapeScaffolder: Structure-Aware 3D Shape Generation from Text | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_ShapeScaffolder_Structure-Aware_3D_Shape_Generation_from_Text_ICCV_2023_paper.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://www.yongliangyang.net/docs/shapescaffolder_iccv23.pdf) | :heavy_minus_sign: |
| SuS-X: Training-Free Name-Only Transfer of Vision-Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vishaal27.github.io/SuS-X-webpage/) <br /> [![GitHub](https://img.shields.io/github/stars/vishaal27/SuS-X)](https://github.com/vishaal27/SuS-X) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16198-b31b1b.svg)](https://arxiv.org/abs/2211.16198) | :heavy_minus_sign: |
| X-Mesh: Towards Fast and Accurate Text-Driven 3D Stylization via Dynamic Textual Guidance | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xmu-xiaoma666.github.io/Projects/X-Mesh/) <br /> [![GitHub](https://img.shields.io/github/stars/xmu-xiaoma666/X-Mesh)](https://github.com/xmu-xiaoma666/X-Mesh) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_X-Mesh_Towards_Fast_and_Accurate_Text-driven_3D_Stylization_via_Dynamic_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15764-b31b1b.svg)](https://arxiv.org/abs/2303.15764) | :heavy_minus_sign: |
| OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation | [![GitHub](https://img.shields.io/github/stars/wudongming97/OnlineRefer)](https://github.com/wudongming97/OnlineRefer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_OnlineRefer_A_Simple_Online_Baseline_for_Referring_Video_Object_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09356-b31b1b.svg)](https://arxiv.org/abs/2307.09356) | :heavy_minus_sign: |
| Attentive Mask CLIP | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Attentive_Mask_CLIP_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08653-b31b1b.svg)](https://arxiv.org/abs/2212.08653) | :heavy_minus_sign: |
| Knowledge Proxy Intervention for Deconfounded Video Question Answering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Knowledge_Proxy_Intervention_for_Deconfounded_Video_Question_Answering_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| UniVTG: Towards Unified Video-Language Temporal Grounding | [![GitHub](https://img.shields.io/github/stars/showlab/UniVTG)](https://github.com/showlab/UniVTG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_UniVTG_Towards_Unified_Video-Language_Temporal_Grounding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.16715-b31b1b.svg)](https://arxiv.org/abs/2307.16715) | :heavy_minus_sign: |
| Self-Supervised Cross-View Representation Reconstruction for Change Captioning | [![GitHub](https://img.shields.io/github/stars/tuyunbin/SCORER)](https://github.com/tuyunbin/SCORER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tu_Self-supervised_Cross-view_Representation_Reconstruction_for_Change_Captioning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Unified Coarse-to-Fine Alignment for Video-Text Retrieval | [![GitHub](https://img.shields.io/github/stars/Ziyang412/UCoFiA)](https://github.com/Ziyang412/UCoFiA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Unified_Coarse-to-Fine_Alignment_for_Video-Text_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.10091-b31b1b.svg)](https://arxiv.org/abs/2309.10091) | :heavy_minus_sign: |
| Confidence-Aware Pseudo-Label Learning for Weakly Supervised Visual Grounding | [![GitHub](https://img.shields.io/github/stars/zjh31/CPL)](https://github.com/zjh31/CPL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Confidence-aware_Pseudo-label_Learning_for_Weakly_Supervised_Visual_Grounding_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TextPSG: Panoptic Scene Graph Generation from Textual Descriptions | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vis-www.cs.umass.edu/TextPSG/) <br /> [![GitHub](https://img.shields.io/github/stars/chengyzhao/TextPSG)](https://github.com/chengyzhao/TextPSG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_TextPSG_Panoptic_Scene_Graph_Generation_from_Textual_Descriptions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.07056-b31b1b.svg)](https://arxiv.org/abs/2310.07056) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_ZjMXMKjm58) |
| MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wlin-at.github.io/maxi) <br /> [![GitHub](https://img.shields.io/github/stars/wlin-at/MAXI)](https://github.com/wlin-at/MAXI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_MAtch_eXpand_and_Improve_Unsupervised_Finetuning_for_Zero-Shot_Action_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08914-b31b1b.svg)](https://arxiv.org/abs/2303.08914) | :heavy_minus_sign: |
| Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Unify_Align_and_Refine_Multi-Level_Semantic_Alignment_for_Radiology_Report_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15932-b31b1b.svg)](https://arxiv.org/abs/2303.15932) | :heavy_minus_sign: |
| CLIPTrans: Transferring Visual Knowledge with Pre-Trained Models for Multimodal Machine Translation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://devaansh100.github.io/projects/cliptrans/) <br /> [![GitHub](https://img.shields.io/github/stars/devaansh100/CLIPTrans)](https://github.com/devaansh100/CLIPTrans) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gupta_CLIPTrans_Transferring_Visual_Knowledge_with_Pre-trained_Models_for_Multimodal_Machine_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.15226-b31b1b.svg)](https://arxiv.org/abs/2308.15226) | :heavy_minus_sign: |
| Learning Human-Human Interactions in Images from Weak Textual Supervision | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tau-vailab.github.io/learning-interactions/) <br /> [![GitHub](https://img.shields.io/github/stars/TAU-VAILab/learning-interactions)](https://github.com/TAU-VAILab/learning-interactions) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Alper_Learning_Human-Human_Interactions_in_Images_from_Weak_Textual_Supervision_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.14104-b31b1b.svg)](https://arxiv.org/abs/2304.14104) | :heavy_minus_sign: |
| BUS: Efficient and Effective Vision-Language Pre-Training with Bottom-Up Patch Summarization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_BUS_Efficient_and_Effective_Vision-Language_Pre-Training_with_Bottom-Up_Patch_Summarization._ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08504-b31b1b.svg)](https://arxiv.org/abs/2307.08504) | :heavy_minus_sign: |
| 3D-VisTA: Pre-Trained Transformer for 3D Vision and Text Alignment | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3d-vista.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/3d-vista/3D-VisTA)](https://github.com/3d-vista/3D-VisTA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_3D-VisTA_Pre-trained_Transformer_for_3D_Vision_and_Text_Alignment_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04352-b31b1b.svg)](https://arxiv.org/abs/2308.04352) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uUtMaoif8DQ&t=1s) |
| ALIP: Adaptive Language-Image Pre-Training with Synthetic Caption | [![GitHub](https://img.shields.io/github/stars/deepglint/ALIP)](https://github.com/deepglint/ALIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_ALIP_Adaptive_Language-Image_Pre-Training_with_Synthetic_Caption_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08428-b31b1b.svg)](https://arxiv.org/abs/2308.08428) | :heavy_minus_sign: |
| LoGoPrompt: Synthetic Text Images can be Good Visual Prompts for Vision-Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chengshiest.github.io/logo/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01155-b31b1b.svg)](https://arxiv.org/abs/2309.01155) | :heavy_minus_sign: |
| Noise-Aware Learning from Web-Crawled Image-Text Data for Image Captioning | [![GitHub](https://img.shields.io/github/stars/kakaobrain/noc)](https://github.com/kakaobrain/noc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Noise-Aware_Learning_from_Web-Crawled_Image-Text_Data_for_Image_Captioning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.13563-b31b1b.svg)](https://arxiv.org/abs/2212.13563) | :heavy_minus_sign: |
| Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Decouple_Before_Interact_Multi-Modal_Prompt_Learning_for_Continual_Visual_Question_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3 | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yushi-hu.github.io/promptcap_demo/) <br /> [![GitHub](https://img.shields.io/github/stars/Yushi-Hu/PromptCap)](https://github.com/Yushi-Hu/PromptCap) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_PromptCap_Prompt-Guided_Image_Captioning_for_VQA_with_GPT-3_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09699-b31b1b.svg)](https://arxiv.org/abs/2211.09699) | :heavy_minus_sign: |
| Grounded Image Text Matching with Mismatched Relation Reasoning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://weiyana.github.io/pages/dataset.html) <br /> [![GitHub](https://img.shields.io/github/stars/SHTUPLUS/GITM-MR)](https://github.com/SHTUPLUS/GITM-MR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Grounded_Image_Text_Matching_with_Mismatched_Relation_Reasoning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.01236-b31b1b.svg)](https://arxiv.org/abs/2308.01236) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eHXm2LrSSqE) |
| GePSAn: Generative Procedure Step Anticipation in Cooking Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Abdelsalam_GePSAn_Generative_Procedure_Step_Anticipation_in_Cooking_Videos_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dki-lab.github.io/LLM-Planner/) <br /> [![GitHub](https://img.shields.io/github/stars/OSU-NLP-Group/LLM-Planner)](https://github.com/OSU-NLP-Group/LLM-Planner) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04088-b31b1b.svg)](https://arxiv.org/abs/2212.04088) | :heavy_minus_sign: |
| VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control | [![GitHub](https://img.shields.io/github/stars/HenryHZY/VL-PET)](https://github.com/HenryHZY/VL-PET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_VL-PET_Vision-and-Language_Parameter-Efficient_Tuning_via_Granularity_Control_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09804-b31b1b.svg)](https://arxiv.org/abs/2308.09804) | :heavy_minus_sign: |
| With a Little Help from Your own Past: Prototypical Memory Networks for Image Captioning | [![GitHub](https://img.shields.io/github/stars/aimagelab/PMA-Net)](https://github.com/aimagelab/PMA-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Barraco_With_a_Little_Help_from_Your_Own_Past_Prototypical_Memory_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12383-b31b1b.svg)](https://arxiv.org/abs/2308.12383) | :heavy_minus_sign: |
| DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models | [![GitHub](https://img.shields.io/github/stars/j-min/DallEval)](https://github.com/j-min/DallEval) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_DALL-Eval_Probing_the_Reasoning_Skills_and_Social_Biases_of_Text-to-Image_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2202.04053-b31b1b.svg)](https://arxiv.org/abs/2202.04053) | :heavy_minus_sign: |
| Learning Navigational Visual Representations with Semantic Map Supervision | [![GitHub](https://img.shields.io/github/stars/YicongHong/Ego2Map-NaViT)](https://github.com/YicongHong/Ego2Map-NaViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12335-b31b1b.svg)](https://arxiv.org/abs/2307.12335) | :heavy_minus_sign: |
| CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://toneyaya.github.io/cotdet/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_CoTDet_Affordance_Knowledge_Prompting_for_Task_Driven_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01093-b31b1b.svg)](https://arxiv.org/abs/2309.01093) | :heavy_minus_sign: |
| Open Set Video HOI detection from Action-Centric Chain-of-Look Prompting | [![GitHub](https://img.shields.io/github/stars/southnx/ACoLP)](https://github.com/southnx/ACoLP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xi_Open_Set_Video_HOI_detection_from_Action-Centric_Chain-of-Look_Prompting_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Learning Concise and Descriptive Attributes for Visual Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Learning_Concise_and_Descriptive_Attributes_for_Visual_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03685-b31b1b.svg)](https://arxiv.org/abs/2308.03685) | :heavy_minus_sign: |
| Open-Vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models | [![GitHub](https://img.shields.io/github/stars/mlvlab/OVQA)](https://github.com/mlvlab/OVQA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ko_Open-vocabulary_Video_Question_Answering_A_New_Benchmark_for_Evaluating_the_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09363-b31b1b.svg)](https://arxiv.org/abs/2308.09363) | :heavy_minus_sign: |
| Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/google-research/google-research/tree/master/encyclopedic_vqa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mensink_Encyclopedic_VQA_Visual_Questions_About_Detailed_Properties_of_Fine-Grained_Categories_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09224-b31b1b.svg)](https://arxiv.org/abs/2306.09224) | :heavy_minus_sign: |
| Story Visualization by Online Text Augmentation with Context Memory | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dcahn12.github.io/projects/CMOTA/) <br /> [![GitHub](https://img.shields.io/github/stars/yonseivnl/cmota)](https://github.com/yonseivnl/cmota) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ahn_Story_Visualization_by_Online_Text_Augmentation_with_Context_Memory_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07575-b31b1b.svg)](https://arxiv.org/abs/2308.07575) | :heavy_minus_sign: |
| Transferable Decoding with Visual Entities for Zero-Shot Image Captioning | [![GitHub](https://img.shields.io/github/stars/FeiElysia/ViECap)](https://github.com/FeiElysia/ViECap) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fei_Transferable_Decoding_with_Visual_Entities_for_Zero-Shot_Image_Captioning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.16525-b31b1b.svg)](https://arxiv.org/abs/2307.16525) | :heavy_minus_sign: |
| Too Large; Data Reduction for Vision-Language Pre-Training | [![GitHub](https://img.shields.io/github/stars/showlab/datacentric.vlp)](https://github.com/showlab/datacentric.vlp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Too_Large_Data_Reduction_for_Vision-Language_Pre-Training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.20087-b31b1b.svg)](https://arxiv.org/abs/2305.20087) | :heavy_minus_sign: |
| ViLTA: Enhancing Vision-Language Pre-Training through Textual Augmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ViLTA_Enhancing_Vision-Language_Pre-training_through_Textual_Augmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16689-b31b1b.svg)](https://arxiv.org/abs/2308.16689) | :heavy_minus_sign: |
| Zero-Shot Composed Image Retrieval with Textual Inversion | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://circo.micc.unifi.it/demo) <br /> [![GitHub](https://img.shields.io/github/stars/miccunifi/SEARLE)](https://github.com/miccunifi/SEARLE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15247-b31b1b.svg)](https://arxiv.org/abs/2303.15247) | :heavy_minus_sign: |
| SATR: Zero-Shot Semantic Segmentation of 3D Shapes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://samir55.github.io/SATR/) <br /> [![GitHub](https://img.shields.io/github/stars/Samir55/SATR)](https://github.com/Samir55/SATR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Abdelreheem_SATR_Zero-Shot_Semantic_Segmentation_of_3D_Shapes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04909-b31b1b.svg)](https://arxiv.org/abs/2304.04909) | :heavy_minus_sign: |
| CiT: Curation in Training for Effective Vision-Language Data | [![GitHub](https://img.shields.io/github/stars/facebookresearch/CiT)](https://github.com/facebookresearch/CiT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_CiT_Curation_in_Training_for_Effective_Vision-Language_Data_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02241-b31b1b.svg)](https://arxiv.org/abs/2301.02241) | :heavy_minus_sign: |
| Self-Regulating Prompts: Foundational Model Adaptation without Forgetting | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://muzairkhattak.github.io/PromptSRC/) <br /> [![GitHub](https://img.shields.io/github/stars/muzairkhattak/PromptSRC)](https://github.com/muzairkhattak/PromptSRC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.06948-b31b1b.svg)](https://arxiv.org/abs/2307.06948) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VVLwL57UBDg) |
| Learning to Ground Instructional Articles in Videos through Narrations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mavroudi_Learning_to_Ground_Instructional_Articles_in_Videos_through_Narrations_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.03802-b31b1b.svg)](https://arxiv.org/abs/2306.03802) | :heavy_minus_sign: |
| RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kurita_RefEgo_Referring_Expression_Comprehension_Dataset_from_First-Person_Perception_of_Ego4D_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12035-b31b1b.svg)](https://arxiv.org/abs/2308.12035) | :heavy_minus_sign: |
| Multi3DRefer: Grounding Text Description to Multiple 3D Objects | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3dlg-hcvc.github.io/multi3drefer/) <br /> [![GitHub](https://img.shields.io/github/stars/3dlg-hcvc/M3DRef-CLIP)](https://github.com/3dlg-hcvc/M3DRef-CLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Multi3DRefer_Grounding_Text_Description_to_Multiple_3D_Objects_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.05251-b31b1b.svg)](https://arxiv.org/abs/2309.05251) | :heavy_minus_sign: |
| Bayesian Prompt Learning for Image-Language Model Generalization | [![GitHub](https://img.shields.io/github/stars/saic-fi/Bayesian-Prompt-Learning)](https://github.com/saic-fi/Bayesian-Prompt-Learning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.02390-b31b1b.svg)](https://arxiv.org/abs/2210.02390) | :heavy_minus_sign: |
| Who are You Referring to? Coreference Resolution in Image Narrations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Goel_Who_Are_You_Referring_To_Coreference_Resolution_In_Image_Narrations_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14563-b31b1b.svg)](https://arxiv.org/abs/2211.14563) | :heavy_minus_sign: |
| Guiding Image Captioning Models Toward more Specific Captions | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kornblith_Guiding_Image_Captioning_Models_Toward_More_Specific_Captions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.16686-b31b1b.svg)](https://arxiv.org/abs/2307.16686) | :heavy_minus_sign: |
| PreSTU: Pre-Training for Scene-Text Understanding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kil_PreSTU_Pre-Training_for_Scene-Text_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.05534-b31b1b.svg)](https://arxiv.org/abs/2209.05534) | :heavy_minus_sign: |
| Exploring Group Video Captioning with Efficient Relational Approximation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Exploring_Group_Video_Captioning_with_Efficient_Relational_Approximation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| VLSlice: Interactive Vision-and-Language Slice Discovery | [![GitHub](https://img.shields.io/github/stars/slymane/vlslice)](https://github.com/slymane/vlslice) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Slyman_VLSlice_Interactive_Vision-and-Language_Slice_Discovery_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06703-b31b1b.svg)](https://arxiv.org/abs/2309.06703) | [![Google Drive](https://img.shields.io/badge/Google%20Drive-4285F4?style=for-the-badge&logo=googledrive&logoColor=white)](https://drive.google.com/file/d/1JkbVXnCds6rOErUx-YWZmp3mQ3IDJuhi/view) |
| Pretrained Language Models as Visual Planners for Human Assistance | [![GitHub](https://img.shields.io/github/stars/facebookresearch/vlamp)](https://github.com/facebookresearch/vlamp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Patel_Pretrained_Language_Models_as_Visual_Planners_for_Human_Assistance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.09179-b31b1b.svg)](https://arxiv.org/abs/2304.09179) | :heavy_minus_sign: |
| VQA Therapy: Exploring Answer Differences by Visually Grounding Answers | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_VQA_Therapy_Exploring_Answer_Differences_by_Visually_Grounding_Answers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11662-b31b1b.svg)](https://arxiv.org/abs/2308.11662) | :heavy_minus_sign: |
| Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation using only Images | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Towards_High-Fidelity_Text-Guided_3D_Face_Generation_and_Manipulation_Using_only_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16758-b31b1b.svg)](https://arxiv.org/abs/2308.16758) | :heavy_minus_sign: |
| Zero-Shot Composed Image Retrieval with Textual Inversion | [![GitHub](https://img.shields.io/github/stars/miccunifi/SEARLE)](https://github.com/miccunifi/SEARLE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15247-b31b1b.svg)](https://arxiv.org/abs/2303.15247) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qxpNb9qxDQI) |
| PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_PatchCT_Aligning_Patch_Set_and_Label_Set_with_Conditional_Transport_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09066-b31b1b.svg)](https://arxiv.org/abs/2307.09066) | :heavy_minus_sign: |
| Lip Reading for Low-Resource Languages by Learning and Combining General Speech Knowledge and Language-Specific Knowledge | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Lip_Reading_for_Low-resource_Languages_by_Learning_and_Combining_General_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09311-b31b1b.svg)](https://arxiv.org/abs/2308.09311) | :heavy_minus_sign: |
| ViewRefer: Grasp the Multi-View Knowledge for 3D Visual Grounding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_ViewRefer_Grasp_the_Multi-view_Knowledge_for_3D_Visual_Grounding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16894-b31b1b.svg)](https://arxiv.org/abs/2303.16894) | :heavy_minus_sign: |
| AerialVLN: Vision-and-Language Navigation for UAVs | [![GitHub](https://img.shields.io/github/stars/AirVLN/AirVLN)](https://github.com/AirVLN/AirVLN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_AerialVLN_Vision-and-Language_Navigation_for_UAVs_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06735-b31b1b.svg)](https://arxiv.org/abs/2308.06735) | :heavy_minus_sign: |
| Linear Spaces of Meanings: Compositional Structures in Vision-Language Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Trager_Linear_Spaces_of_Meanings_Compositional_Structures_in_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14383-b31b1b.svg)](https://arxiv.org/abs/2302.14383) | :heavy_minus_sign: |
| HiTeA: Hierarchical Temporal-Aware Video-Language Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_HiTeA_Hierarchical_Temporal-Aware_Video-Language_Pre-training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14546-b31b1b.svg)](https://arxiv.org/abs/2212.14546) | :heavy_minus_sign: |
| EgoTV: Egocentric Task Verification from Natural Language Task Descriptions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rishihazra.github.io/EgoTV/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/EgoTV)](https://github.com/facebookresearch/EgoTV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hazra_EgoTV_Egocentric_Task_Verification_from_Natural_Language_Task_Descriptions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16975-b31b1b.svg)](https://arxiv.org/abs/2303.16975) | :heavy_minus_sign: |
| SINC: Self-Supervised in-Context Learning for Vision-Language Tasks | [![GitHub](https://img.shields.io/github/stars/YiSyuanChen/SINC)](https://github.com/YiSyuanChen/SINC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_SINC_Self-Supervised_In-Context_Learning_for_Vision-Language_Tasks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07742-b31b1b.svg)](https://arxiv.org/abs/2307.07742) | :heavy_minus_sign: |
| VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation | [![GitHub](https://img.shields.io/github/stars/YanyuanQiao/VLN-PETL)](https://github.com/YanyuanQiao/VLN-PETL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_VLN-PETL_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Navigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10172-b31b1b.svg)](https://arxiv.org/abs/2308.10172) | :heavy_minus_sign: |
| Going Denser with Open-Vocabulary Part Segmentation | [![GitHub](https://img.shields.io/github/stars/facebookresearch/VLPart)](https://github.com/facebookresearch/VLPart) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Going_Denser_with_Open-Vocabulary_Part_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11173-b31b1b.svg)](https://arxiv.org/abs/2305.11173) | :heavy_minus_sign: |
| Temporal Collection and Distribution for Referring Video Object Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://toneyaya.github.io/tempcd/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Temporal_Collection_and_Distribution_for_Referring_Video_Object_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.03473-b31b1b.svg)](https://arxiv.org/abs/2309.03473) | :heavy_minus_sign: |
| Inverse Compositional Learning for Weakly-Supervised Relation Grounding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Inverse_Compositional_Learning_for_Weakly-supervised_Relation_Grounding_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Why is Prompt Tuning for Vision-Language Models Robust to Noisy Labels? | [![GitHub](https://img.shields.io/github/stars/CEWu/PTNL)](https://github.com/CEWu/PTNL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Why_Is_Prompt_Tuning_for_Vision-Language_Models_Robust_to_Noisy_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11978-b31b1b.svg)](https://arxiv.org/abs/2307.11978) | :heavy_minus_sign: |
| CHAMPAGNE: Learning Real-World Conversation from Large-Scale Web Videos | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://seungjuhan.me/champagne/) <br /> [![GitHub](https://img.shields.io/github/stars/wade3han/champagne)](https://github.com/wade3han/champagne) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_CHAMPAGNE_Learning_Real-world_Conversation_from_Large-Scale_Web_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09713-b31b1b.svg)](https://arxiv.org/abs/2303.09713) | :heavy_minus_sign: |
| RCA-NOC: Relative Contrastive Alignment for Novel Object Captioning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_RCA-NOC_Relative_Contrastive_Alignment_for_Novel_Object_Captioning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DIME-FM: DIstilling Multimodal and Efficient Foundation Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cs-people.bu.edu/sunxm/DIME-FM/) <br /> [![GitHub](https://img.shields.io/github/stars/sunxm2357/DIME-FM)](https://github.com/sunxm2357/DIME-FM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_DIME-FM__DIstilling_Multimodal_and_Efficient_Foundation_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.18232-b31b1b.svg)](https://arxiv.org/abs/2303.18232) | :heavy_minus_sign: |
| Black Box Few-Shot Adaptation for Vision-Language Models | [![GitHub](https://img.shields.io/github/stars/saic-fi/LFA)](https://github.com/saic-fi/LFA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01752-b31b1b.svg)](https://arxiv.org/abs/2304.01752) | :heavy_minus_sign: |
| Shatter and Gather: Learning Referring Image Segmentation with Text Supervision | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://southflame.github.io/sag/) <br /> [![GitHub](https://img.shields.io/github/stars/kdwonn/SaG)](https://github.com/kdwonn/SaG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Shatter_and_Gather_Learning_Referring_Image_Segmentation_with_Text_Supervision_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.15512-b31b1b.svg)](https://arxiv.org/abs/2308.15512) | :heavy_minus_sign: |
| Accurate and Fast Compressed Video Captioning | [![GitHub](https://img.shields.io/github/stars/acherstyx/CoCap)](https://github.com/acherstyx/CoCap) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shen_Accurate_and_Fast_Compressed_Video_Captioning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.12867-b31b1b.svg)](https://arxiv.org/abs/2309.12867) | :heavy_minus_sign: |
| Exploring Temporal Concurrency for Video-Language Representation Learning | [![GitHub](https://img.shields.io/github/stars/hengRUC/TCP)](https://github.com/hengRUC/TCP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Exploring_Temporal_Concurrency_for_Video-Language_Representation_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Verbs in Action: Improving Verb Understanding in Video-Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/google-research/scenic/tree/main/scenic/projects/verbs_in_action) <br /> [![GitHub](https://img.shields.io/github/stars/google-research/scenic)](https://github.com/google-research/scenic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Momeni_Verbs_in_Action_Improving_Verb_Understanding_in_Video-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06708-b31b1b.svg)](https://arxiv.org/abs/2304.06708) | :heavy_minus_sign: |
| Sign Language Translation with Iterative Prototype | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Sign_Language_Translation_with_Iterative_Prototype_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12191-b31b1b.svg)](https://arxiv.org/abs/2308.12191) | :heavy_minus_sign: |
| Contrastive Feature Masking Open-Vocabulary Vision Transformer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Contrastive_Feature_Masking_Open-Vocabulary_Vision_Transformer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00775-b31b1b.svg)](https://arxiv.org/abs/2309.00775) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9dH4LpStK-0) |
| Toward Unsupervised Realistic Visual Question Answering | [![GitHub](https://img.shields.io/github/stars/chihhuiho/RGQA)](https://github.com/chihhuiho/RGQA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Toward_Unsupervised_Realistic_Visual_Question_Answering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05068-b31b1b.svg)](https://arxiv.org/abs/2303.05068) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=APPK_9DzpXE) |
| GridMM: Grid Memory Map for Vision-and-Language Navigation | [![GitHub](https://img.shields.io/github/stars/MrZihan/GridMM)](https://github.com/MrZihan/GridMM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_GridMM_Grid_Memory_Map_for_Vision-and-Language_Navigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12907-b31b1b.svg)](https://arxiv.org/abs/2307.12907) | :heavy_minus_sign: |
| Video Background Music Generation: Dataset, Method and Evaluation | [![GitHub](https://img.shields.io/github/stars/zhuole1025/SymMV)](https://github.com/zhuole1025/SymMV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhuo_Video_Background_Music_Generation_Dataset_Method_and_Evaluation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11248-b31b1b.svg)](https://arxiv.org/abs/2211.11248) | :heavy_minus_sign: |
| Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval | [![GitHub](https://img.shields.io/github/stars/bladewaltz1/PromptSwitch)](https://github.com/bladewaltz1/PromptSwitch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_Prompt_Switch_Efficient_CLIP_Adaptation_for_Text-Video_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07648-b31b1b.svg)](https://arxiv.org/abs/2308.07648) | :heavy_minus_sign: |
| Prompt-Aligned Gradient for Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/BeierZhu/Prompt-align)](https://github.com/BeierZhu/Prompt-align) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.14865-b31b1b.svg)](https://arxiv.org/abs/2205.14865) | :heavy_minus_sign: |
| Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11186-b31b1b.svg)](https://arxiv.org/abs/2308.11186) | :heavy_minus_sign: |
| Order-Prompted Tag Sequence Generation for Video Tagging | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Order-Prompted_Tag_Sequence_Generation_for_Video_Tagging_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| What does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification | [![GitHub](https://img.shields.io/github/stars/sarahpratt/CuPL)](https://github.com/sarahpratt/CuPL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.03320-b31b1b.svg)](https://arxiv.org/abs/2209.03320) | :heavy_minus_sign: |
| PromptStyler: Prompt-Driven Style Generation for Source-Free Domain Generalization | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://promptstyler.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15199-b31b1b.svg)](https://arxiv.org/abs/2307.15199) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0PsU4pbW0mQ) |
| DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09306-b31b1b.svg)](https://arxiv.org/abs/2308.09306) | :heavy_minus_sign: |
| EdaDet: Open-Vocabulary Object Detection using Early Dense Alignment | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chengshiest.github.io/edadet/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_EdaDet_Open-Vocabulary_Object_Detection_Using_Early_Dense_Alignment_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01151-b31b1b.svg)](https://arxiv.org/abs/2309.01151) | :heavy_minus_sign: |
| MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition | [![GitHub](https://img.shields.io/github/stars/Exgc/AVMuST-TED)](https://github.com/Exgc/AVMuST-TED) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_MixSpeech_Cross-Modality_Self-Learning_with_Audio-Visual_Stream_Mixup_for_Visual_Speech_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05309-b31b1b.svg)](https://arxiv.org/abs/2303.05309) | :heavy_minus_sign: |
| Waffling Around for Performance: Visual Classification with Random Words and Broad Concepts | [![GitHub](https://img.shields.io/github/stars/ExplainableML/WaffleCLIP)](https://github.com/ExplainableML/WaffleCLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07282-b31b1b.svg)](https://arxiv.org/abs/2306.07282) | :heavy_minus_sign: |
| March in Chat: Interactive Prompting for Remote Embodied Referring Expression | [![GitHub](https://img.shields.io/github/stars/YanyuanQiao/MiC)](https://github.com/YanyuanQiao/MiC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10141-b31b1b.svg)](https://arxiv.org/abs/2308.10141) | :heavy_minus_sign: |
| Chinese Text Recognition with a Pre-Trained CLIP-Like Model through Image-IDS Aligning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR) <br /> [![GitHub](https://img.shields.io/github/stars/FudanVI/FudanOCR)](https://github.com/FudanVI/FudanOCR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Chinese_Text_Recognition_with_A_Pre-Trained_CLIP-Like_Model_Through_Image-IDS_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01083-b31b1b.svg)](https://arxiv.org/abs/2309.01083) | :heavy_minus_sign: |
| OmniLabel: A Challenging Benchmark for Language-based Object Detection | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.omnilabel.org/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Schulter_OmniLabel_A_Challenging_Benchmark_for_Language-Based_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11463-b31b1b.svg)](https://arxiv.org/abs/2304.11463) | :heavy_minus_sign: |
| IntentQA: Context-Aware Video Intent Reasoning | [![GitHub](https://img.shields.io/github/stars/JoseponLee/IntentQA)](https://github.com/JoseponLee/IntentQA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_IntentQA_Context-aware_Video_Intent_Reasoning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Sigmoid Loss for Language Image Pre-Training | [![GitHub](https://img.shields.io/github/stars/google-research/big_vision)](https://github.com/google-research/big_vision) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15343-b31b1b.svg)](https://arxiv.org/abs/2303.15343) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=H4yPlDPomrI) |
| What does CLIP Know About a Red Circle? Visual Prompt Engineering for VLMs | [![GitHub](https://img.shields.io/github/stars/suny-sht/clip-red-circle)](https://github.com/suny-sht/clip-red-circle) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shtedritski_What_does_CLIP_know_about_a_red_circle_Visual_prompt_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06712-b31b1b.svg)](https://arxiv.org/abs/2304.06712) | :heavy_minus_sign: |
| Equivariant Similarity for Vision-Language Foundation Models | [![GitHub](https://img.shields.io/github/stars/Wangt-CN/EqBen)](https://github.com/Wangt-CN/EqBen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Equivariant_Similarity_for_Vision-Language_Foundation_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14465-b31b1b.svg)](https://arxiv.org/abs/2303.14465) | :heavy_minus_sign: |
| Scaling Data Generation in Vision-and-Language Navigation | [![GitHub](https://img.shields.io/github/stars/wz0919/ScaleVLN)](https://github.com/wz0919/ScaleVLN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Scaling_Data_Generation_in_Vision-and-Language_Navigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15644-b31b1b.svg)](https://arxiv.org/abs/2307.15644) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QCGWSM_okfM) |
| Name Your Colour for the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer | [![GitHub](https://img.shields.io/github/stars/ryeocthiv/CQFormer)](https://github.com/ryeocthiv/CQFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Su_Name_Your_Colour_For_the_Task_Artificially_Discover_Colour_Naming_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03434-b31b1b.svg)](https://arxiv.org/abs/2212.03434) | :heavy_minus_sign: |
| G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_G2L_Semantically_Aligned_and_Uniform_Video_Grounding_via_Geodesic_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14277-b31b1b.svg)](https://arxiv.org/abs/2307.14277) | :heavy_minus_sign: |
| Grounded Entity-Landmark Adaptive Pre-Training for Vision-and-Language Navigation | [![GitHub](https://img.shields.io/github/stars/CSir1996/VLN-GELA)](https://github.com/CSir1996/VLN-GELA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cui_Grounded_Entity-Landmark_Adaptive_Pre-Training_for_Vision-and-Language_Navigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12587-b31b1b.svg)](https://arxiv.org/abs/2308.12587) | :heavy_minus_sign: |
| Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ibrahimi_Audio-Enhanced_Text-to-Video_Retrieval_using_Text-Conditioned_Feature_Alignment_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12964-b31b1b.svg)](https://arxiv.org/abs/2307.12964) | :heavy_minus_sign: |
| Open-Domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://open-vision-language.github.io/oven/) <br /> [![GitHub](https://img.shields.io/github/stars/edchengg/oven_eval)](https://github.com/edchengg/oven_eval) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.11154-b31b1b.svg)](https://arxiv.org/abs/2302.11154) | :heavy_minus_sign: |

<!-- | BEVBert: Multimodal Map Pre-Training for Language-Guided Navigation | [![GitHub](https://img.shields.io/github/stars/MarSaKi/VLN-BEVBert)](https://github.com/MarSaKi/VLN-BEVBert) | [![arXiv](https://img.shields.io/badge/arXiv-2212.04385-b31b1b.svg)](https://arxiv.org/abs/2212.04385) | :heavy_minus_sign: | -->
<!-- | Improving Zero-Shot Generalization for CLIP with Synthesized Prompts | [![GitHub](https://img.shields.io/github/stars/mrflogs/SHIP)](https://github.com/mrflogs/SHIP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07397-b31b1b.svg)](https://arxiv.org/abs/2307.07397) | :heavy_minus_sign: | -->
