# ICCV-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-vision-and-theory.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-a-single-image-and-shape-from-x.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
    </a>
</div>

## Navigation and Autonomous Driving

![Section Papers](https://img.shields.io/badge/Section%20Papers-51-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-45-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-29-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-5-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Robust Monocular Depth Estimation under Challenging Conditions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://md4all.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/md4all/md4all?style=flat)](https://github.com/md4all/md4all) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gasperini_Robust_Monocular_Depth_Estimation_under_Challenging_Conditions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09711-b31b1b.svg)](https://arxiv.org/abs/2308.09711) | :heavy_minus_sign: |
| UMC: A Unified Bandwidth-Efficient and Multi-Resolution based Collaborative Perception Framework | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tianhangwang.github.io/UMC/) <br /> [![GitHub](https://img.shields.io/github/stars/ispc-lab/UMC?style=flat)](https://github.com/ispc-lab/UMC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_UMC_A_Unified_Bandwidth-efficient_and_Multi-resolution_based_Collaborative_Perception_Framework_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12400-b31b1b.svg)](https://arxiv.org/abs/2303.12400) | :heavy_minus_sign: |
| View Consistent Purification for Accurate Cross-View Localization | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shanwang-shan.github.io/PureACL-website/) <br /> [![GitHub](https://img.shields.io/github/stars/ShanWang-Shan/PureACL-website?style=flat)](https://github.com/ShanWang-Shan/PureACL-website) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_View_Consistent_Purification_for_Accurate_Cross-View_Localization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08110-b31b1b.svg)](https://arxiv.org/abs/2308.08110) | :heavy_minus_sign: |
| Semi-Supervised Semantics-Guided Adversarial Training for Robust Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/jrcblue/SSAT-for-Motion-Prediction?style=flat)](https://github.com/jrcblue/SSAT-for-Motion-Prediction) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiao_Semi-supervised_Semantics-guided_Adversarial_Training_for_Robust_Trajectory_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.14230-b31b1b.svg)](https://arxiv.org/abs/2205.14230) | :heavy_minus_sign: |
| NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping | [![GitHub](https://img.shields.io/github/stars/JunyuanDeng/NeRF-LOAM?style=flat)](https://github.com/JunyuanDeng/NeRF-LOAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_NeRF-LOAM_Neural_Implicit_Representation_for_Large-Scale_Incremental_LiDAR_Odometry_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10709-b31b1b.svg)](https://arxiv.org/abs/2303.10709) | :heavy_minus_sign: |
| MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mapprior.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/xiyuez2/MapPrior?style=flat)](https://github.com/xiyuez2/MapPrior) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_MapPrior_Birds-Eye_View_Map_Layout_Estimation_with_Generative_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12963-b31b1b.svg)](https://arxiv.org/abs/2308.12963) | :heavy_minus_sign: |
| Hidden Biases of End-to-End Driving Models | [![GitHub](https://img.shields.io/github/stars/autonomousvision/carla_garage?style=flat)](https://github.com/autonomousvision/carla_garage) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jaeger_Hidden_Biases_of_End-to-End_Driving_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07957-b31b1b.svg)](https://arxiv.org/abs/2306.07957) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ChrPW8RdqQU) |
| Search for or Navigate to? Dual Adaptive Thinking for Object Navigation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dang_Search_for_or_Navigate_to_Dual_Adaptive_Thinking_for_Object_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.00553-b31b1b.svg)](https://arxiv.org/abs/2208.00553) | :heavy_minus_sign: |
| BiFF: Bi-Level Future Fusion with Polyline-based Coordinate for Interactive Trajectory Prediction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_BiFF_Bi-level_Future_Fusion_with_Polyline-based_Coordinate_for_Interactive_Trajectory_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.14161-b31b1b.svg)](https://arxiv.org/abs/2306.14161) | :heavy_minus_sign: |
| Towards Zero Domain Gap: A Comprehensive Study of Realistic LiDAR Simulation for Autonomy Testing | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://waabi.ai/lidar-dg/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Manivasagam_Towards_Zero_Domain_Gap_A_Comprehensive_Study_of_Realistic_LiDAR_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Clustering based Point Cloud Representation Learning for 3D Analysis | [![GitHub](https://img.shields.io/github/stars/FengZicai/Cluster3Dseg?style=flat)](https://github.com/FengZicai/Cluster3Dseg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Clustering_based_Point_Cloud_Representation_Learning_for_3D_Analysis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14605-b31b1b.svg)](https://arxiv.org/abs/2307.14605) | :heavy_minus_sign: |
| ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kuis-ai.github.io/adapt/) <br /> [![GitHub](https://img.shields.io/github/stars/KUIS-AI/adapt?style=flat)](https://github.com/KUIS-AI/adapt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Aydemir_ADAPT_Efficient_Multi-Agent_Trajectory_Prediction_with_Adaptation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14187-b31b1b.svg)](https://arxiv.org/abs/2307.14187) | :heavy_minus_sign: |
| MV-DeepSDF: Implicit Modeling with Multi-Sweep Point Clouds for 3D Vehicle Reconstruction in Autonomous Driving | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_MV-DeepSDF_Implicit_Modeling_with_Multi-Sweep_Point_Clouds_for_3D_Vehicle_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16715-b31b1b.svg)](https://arxiv.org/abs/2309.16715) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=k9RbDA1nE7s) |
| Learning Vision-and-Language Navigation from YouTube Videos | [![GitHub](https://img.shields.io/github/stars/JeremyLinky/YouTube-VLN?style=flat)](https://github.com/JeremyLinky/YouTube-VLN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Learning_Vision-and-Language_Navigation_from_YouTube_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11984-b31b1b.svg)](https://arxiv.org/abs/2307.11984) | :heavy_minus_sign: |
| TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_TrajPAC_Towards_Robustness_Verification_of_Pedestrian_Trajectory_Prediction_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05985-b31b1b.svg)](https://arxiv.org/abs/2308.05985) | :heavy_minus_sign: |
| VAD: Vectorized Scene Representation for Efficient Autonomous Driving | [![GitHub](https://img.shields.io/github/stars/hustvl/VAD?style=flat)](https://github.com/hustvl/VAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12077-b31b1b.svg)](https://arxiv.org/abs/2303.12077) | :heavy_minus_sign: |
| Traj-MAE: Masked Autoencoders for Trajectory Prediction | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://jiazewang.com/projects/trajmae.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Traj-MAE_Masked_Autoencoders_for_Trajectory_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06697-b31b1b.svg)](https://arxiv.org/abs/2303.06697) | :heavy_minus_sign: |
| Sparse Point Guided 3D Lane Detection | [![GitHub](https://img.shields.io/github/stars/YaoChengTang/Sparse-Point-Guided-3D-Lane-Detection?style=flat)](https://github.com/YaoChengTang/Sparse-Point-Guided-3D-Lane-Detection) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Sparse_Point_Guided_3D_Lane_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| A Simple Vision Transformer for Weakly Semi-Supervised 3D Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Learn TAROT with MENTOR: A Meta-Learned Self-Supervised Approach for Trajectory Prediction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pourkeshavarz_Learn_TAROT_with_MENTOR_A_Meta-Learned_Self-Supervised_Approach_for_Trajectory_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| FocalFormer3D: Focusing on Hard Instance for 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/NVlabs/FocalFormer3D?style=flat)](https://github.com/NVlabs/FocalFormer3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_FocalFormer3D_Focusing_on_Hard_Instance_for_3D_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04556-b31b1b.svg)](https://arxiv.org/abs/2308.04556) | :heavy_minus_sign: |
| Scene as Occupancy | [![GitHub](https://img.shields.io/github/stars/OpenDriveLab/OccNet?style=flat)](https://github.com/OpenDriveLab/OccNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tong_Scene_as_Occupancy_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02851-b31b1b.svg)](https://arxiv.org/abs/2306.02851) | :heavy_minus_sign: |
| Real-Time Neural Rasterization for Large Scenes | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://waabi.ai/NeuRas/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Real-Time_Neural_Rasterization_for_Large_Scenes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.05607-b31b1b.svg)](https://arxiv.org/abs/2311.05607) | :heavy_minus_sign: |
| A Game of Bundle Adjustment - Learning Efficient Convergence | [![GitHub](https://img.shields.io/github/stars/amirbelder/A-Game-of-Bundle-Adjustment---Learning-Efficient-Convergence?style=flat)](https://github.com/amirbelder/A-Game-of-Bundle-Adjustment---Learning-Efficient-Convergence) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Belder_A_Game_of_Bundle_Adjustment_-_Learning_Efficient_Convergence_ICCV_2023_paper.pdf) [![arXiv](https://img.shields.io/badge/arXiv-2308.13270-b31b1b.svg)](https://arxiv.org/abs/2308.13270) | :heavy_minus_sign: |
| Efficient Transformer-based 3D Object Detection with Dynamic Token Halting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Efficient_Transformer-based_3D_Object_Detection_with_Dynamic_Token_Halting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05078-b31b1b.svg)](https://arxiv.org/abs/2303.05078) | :heavy_minus_sign: |
| RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration | [![GitHub](https://img.shields.io/github/stars/IRMVLab/RegFormer?style=flat)](https://github.com/IRMVLab/RegFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_RegFormer_An_Efficient_Projection-Aware_Transformer_Network_for_Large-Scale_Point_Cloud_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12384-b31b1b.svg)](https://arxiv.org/abs/2303.12384) | :heavy_minus_sign: |
| CASSPR: Cross Attention Single Scan Place Recognition | [![GitHub](https://img.shields.io/github/stars/Yan-Xia/CASSPR?style=flat)](https://github.com/Yan-Xia/CASSPR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_CASSPR_Cross_Attention_Single_Scan_Place_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12542-b31b1b.svg)](https://arxiv.org/abs/2211.12542) | :heavy_minus_sign: |
| Recursive Video Lane Detection | [![GitHub](https://img.shields.io/github/stars/dongkwonjin/RVLD?style=flat)](https://github.com/dongkwonjin/RVLD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jin_Recursive_Video_Lane_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11106-b31b1b.svg)](https://arxiv.org/abs/2308.11106) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z0FaOqVrN5w) |
| Parametric Depth based Feature Representation Learning for Object Detection and Segmentation in Bird's-Eye View | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Parametric_Depth_Based_Feature_Representation_Learning_for_Object_Detection_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.04106-b31b1b.svg)](https://arxiv.org/abs/2307.04106) | :heavy_minus_sign: |
| SHIFT3D: Synthesizing Hard Inputs for Tricking 3D Detectors | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_SHIFT3D_Synthesizing_Hard_Inputs_For_Tricking_3D_Detectors_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.05810-b31b1b.svg)](https://arxiv.org/abs/2309.05810) | :heavy_minus_sign: |
| Bootstrap Motion Forecasting With Self-Consistent Constraints | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Bootstrap_Motion_Forecasting_With_Self-Consistent_Constraints_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.05859-b31b1b.svg)](https://arxiv.org/abs/2204.05859) | :heavy_minus_sign: |
| Towards Viewpoint Robustness in Bird's Eye View Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nvlabs.github.io/viewpoint-robustness/) <br /> [![GitHub](https://img.shields.io/github/stars/NVlabs/viewpoint-robustness?style=flat)](https://github.com/NVlabs/viewpoint-robustness) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Klinghoffer_Towards_Viewpoint_Robustness_in_Birds_Eye_View_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.05192-b31b1b.svg)](https://arxiv.org/abs/2309.05192) | :heavy_minus_sign: |
| R-Pred: Two-Stage Motion Prediction via Tube-Query Attention-based Trajectory Refinement | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Choi_R-Pred_Two-Stage_Motion_Prediction_Via_Tube-Query_Attention-Based_Trajectory_Refinement_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.08609-b31b1b.svg)](https://arxiv.org/abs/2211.08609) | :heavy_minus_sign: |
| INT2: Interactive Trajectory Prediction at Intersections | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://int2.cn/) <br /> [![GitHub](https://img.shields.io/github/stars/AIR-DISCOVER/INT2?style=flat)](https://github.com/AIR-DISCOVER/INT2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_INT2_Interactive_Trajectory_Prediction_at_Intersections_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KNkuakDvgVc) |
| MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception | [![GitHub](https://img.shields.io/github/stars/ZRandomize/MatrixVT?style=flat)](https://github.com/ZRandomize/MatrixVT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_MatrixVT_Efficient_Multi-Camera_to_BEV_Transformation_for_3D_Perception_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10593-b31b1b.svg)](https://arxiv.org/abs/2211.10593) | :heavy_minus_sign: |
| Unsupervised Self-Driving Attention Prediction via Uncertainty Mining and Knowledge Embedding | [![GitHub](https://img.shields.io/github/stars/zaplm/DriverAttention?style=flat)](https://github.com/zaplm/DriverAttention) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Unsupervised_Self-Driving_Attention_Prediction_via_Uncertainty_Mining_and_Knowledge_Embedding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09706-b31b1b.svg)](https://arxiv.org/abs/2303.09706) | :heavy_minus_sign: |
| SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_SVQNet_Sparse_Voxel-Adjacent_Query_Network_for_4D_Spatio-Temporal_LiDAR_Semantic_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13323-b31b1b.svg)](https://arxiv.org/abs/2308.13323) | :heavy_minus_sign: |
| MotionLM: Multi-Agent Motion Forecasting as Language Modeling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Seff_MotionLM_Multi-Agent_Motion_Forecasting_as_Language_Modeling_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16534-b31b1b.svg)](https://arxiv.org/abs/2309.16534) | :heavy_minus_sign: |
| Improving Online Lane Graph Extraction by Object-Lane Clustering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Can_Improving_Online_Lane_Graph_Extraction_by_Object-Lane_Clustering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10947-b31b1b.svg)](https://arxiv.org/abs/2307.10947) | :heavy_minus_sign: |
| Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14491-b31b1b.svg)](https://arxiv.org/abs/2309.14491) | :heavy_minus_sign: |
| Self-Supervised Monocular Depth Estimation by Direction-Aware Cumulative Convolution Network | [![GitHub](https://img.shields.io/github/stars/wencheng256/DaCCN?style=flat)](https://github.com/wencheng256/DaCCN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Self-Supervised_Monocular_Depth_Estimation_by_Direction-aware_Cumulative_Convolution_Network_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05605-b31b1b.svg)](https://arxiv.org/abs/2308.05605) | :heavy_minus_sign: |
| Ordered Atomic Activity for Fine-Grained Interactive Traffic Scenario Understanding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Agarwal_Ordered_Atomic_Activity_for_Fine-grained_Interactive_Traffic_Scenario_Understanding_ICCV_2023_paper.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1Jwzzr0puAWte5xa-xQwOAnpAXsBsSw7f/view) | :heavy_minus_sign: |
| DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DistillBEV_Boosting_Multi-Camera_3D_Object_Detection_with_Cross-Modal_Knowledge_Distillation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15109-b31b1b.svg)](https://arxiv.org/abs/2309.15109) | :heavy_minus_sign: |
| Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.vis.xyz/pub/vtd/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Video_Task_Decathlon_Unifying_Image_and_Video_Tasks_in_Autonomous_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.04422-b31b1b.svg)](https://arxiv.org/abs/2309.04422) | :heavy_minus_sign: |
| MV-Map: Offboard HD-Map Generation with Multi-View Consistency | [![GitHub](https://img.shields.io/github/stars/ZiYang-xie/MV-Map?style=flat)](https://github.com/ZiYang-xie/MV-Map) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_MV-Map_Offboard_HD-Map_Generation_with_Multi-view_Consistency_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08851-b31b1b.svg)](https://arxiv.org/abs/2305.08851) | :heavy_minus_sign: |
| Towards Universal LiDAR-based 3D Object Detection by Multi-Domain Knowledge Transfer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Towards_Universal_LiDAR-Based_3D_Object_Detection_by_Multi-Domain_Knowledge_Transfer_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Forecast-MAE: Self-Supervised Pre-Training for Motion Forecasting with Masked Autoencoders | [![GitHub](https://img.shields.io/github/stars/jchengai/forecast-mae?style=flat)](https://github.com/jchengai/forecast-mae) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Forecast-MAE_Self-supervised_Pre-training_for_Motion_Forecasting_with_Masked_Autoencoders_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09882-b31b1b.svg)](https://arxiv.org/abs/2308.09882) | :heavy_minus_sign: |
| UniFusion: Unified Multi-View Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View | [![GitHub](https://img.shields.io/github/stars/cfzd/UniFusion?style=flat)](https://github.com/cfzd/UniFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qin_UniFusion_Unified_Multi-View_Fusion_Transformer_for_Spatial-Temporal_Representation_in_Birds-Eye-View_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.08536-b31b1b.svg)](https://arxiv.org/abs/2207.08536) | :heavy_minus_sign: |
| BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images | [![GitHub](https://img.shields.io/github/stars/zjuluolun/BEVPlace?style=flat)](https://github.com/zjuluolun/BEVPlace) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_BEVPlace_Learning_LiDAR-based_Place_Recognition_using_Birds_Eye_View_Images_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14325-b31b1b.svg)](https://arxiv.org/abs/2302.14325) | :heavy_minus_sign: |
| CORE: Cooperative Reconstruction for Multi-Agent Perception | [![GitHub](https://img.shields.io/github/stars/zllxot/CORE?style=flat)](https://github.com/zllxot/CORE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_CORE_Cooperative_Reconstruction_for_Multi-Agent_Perception_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11514-b31b1b.svg)](https://arxiv.org/abs/2307.11514) | :heavy_minus_sign: |
| MetaBEV: Solving Sensor Failures for 3D Detection and Map Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chongjiange.github.io/metabev.html) <br /> [![GitHub](https://img.shields.io/github/stars/ChongjianGE/MetaBEV?style=flat)](https://github.com/ChongjianGE/MetaBEV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_MetaBEV_Solving_Sensor_Failures_for_3D_Detection_and_Map_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.09801-b31b1b.svg)](https://arxiv.org/abs/2304.09801) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TiEQpYq77Xo) |
