# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/scene-analysis-and-understanding.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/human-in-the-loop-computer-vision.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Multimodal Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-30-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-25-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-24-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-3-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| SLAN: Self-Locator Aided Network for Vision-Language Understanding | [![GitHub](https://img.shields.io/github/stars/scok30/SLAN?style=flat)](https://github.com/scok30/SLAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_SLAN_Self-Locator_Aided_Network_for_Vision-Language_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16208-b31b1b.svg)](https://arxiv.org/abs/2211.16208) | :heavy_minus_sign: |
| Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Long_Task-Oriented_Multi-Modal_Mutual_Leaning_for_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17169-b31b1b.svg)](https://arxiv.org/abs/2303.17169) | :heavy_minus_sign: |
| TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/microsoft/Cream/tree/main/TinyCLIP) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/Cream?style=flat)](https://github.com/microsoft/Cream) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_TinyCLIP_CLIP_Distillation_via_Affinity_Mimicking_and_Weight_Inheritance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.12314-b31b1b.svg)](https://arxiv.org/abs/2309.12314) | :heavy_minus_sign: |
| In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval | [![GitHub](https://img.shields.io/github/stars/ninatu/in_style?style=flat)](https://github.com/ninatu/in_style) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shvetsova_In-Style_Bridging_Text_and_Uncurated_Videos_with_Style_Transfer_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.08928-b31b1b.svg)](https://arxiv.org/abs/2309.08928) | :heavy_minus_sign: |
| Preserving Modality Structure Improves Multi-Modal Learning | [![GitHub](https://img.shields.io/github/stars/Swetha5/Multi_Sinkhorn_Knopp?style=flat)](https://github.com/Swetha5/Multi_Sinkhorn_Knopp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Swetha_Preserving_Modality_Structure_Improves_Multi-Modal_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13077-b31b1b.svg)](https://arxiv.org/abs/2308.13077) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KmyFxfUOGcY) |
| Distribution-Aware Prompt Tuning for Vision-Language Models | [![GitHub](https://img.shields.io/github/stars/mlvlab/DAPT?style=flat)](https://github.com/mlvlab/DAPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Distribution-Aware_Prompt_Tuning_for_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.03406-b31b1b.svg)](https://arxiv.org/abs/2309.03406) | :heavy_minus_sign: |
| SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/IranQin/SupFusion?style=flat)](https://github.com/IranQin/SupFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qin_SupFusion_Supervised_LiDAR-Camera_Fusion_for_3D_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07084-b31b1b.svg)](https://arxiv.org/abs/2309.07084) | :heavy_minus_sign: |
| Distribution-Consistent Modal Recovering for Incomplete Multimodal Learning | [![GitHub](https://img.shields.io/github/stars/mdswyz/DiCMoR?style=flat)](https://github.com/mdswyz/DiCMoR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Distribution-Consistent_Modal_Recovering_for_Incomplete_Multimodal_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Fg-T2M_Fine-Grained_Text-Driven_Human_Motion_Generation_via_Diffusion_Model_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06284-b31b1b.svg)](https://arxiv.org/abs/2309.06284) | :heavy_minus_sign: |
| Cross-Modal Orthogonal High-Rank Augmentation for RGB-Event Transformer-Trackers | [![GitHub](https://img.shields.io/github/stars/ZHU-Zhiyu/High-Rank_RGB-Event_Tracker?style=flat)](https://github.com/ZHU-Zhiyu/High-Rank_RGB-Event_Tracker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Cross-Modal_Orthogonal_High-Rank_Augmentation_for_RGB-Event_Transformer-Trackers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.04129-b31b1b.svg)](https://arxiv.org/abs/2307.04129) | :heavy_minus_sign: |
| eP-ALM: Efficient Perceptual Augmentation of Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mshukor.github.io/eP-ALM.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/mshukor/eP-ALM?style=flat)](https://github.com/mshukor/eP-ALM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shukor_eP-ALM_Efficient_Perceptual_Augmentation_of_Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11403-b31b1b.svg)](https://arxiv.org/abs/2303.11403) | :heavy_minus_sign: |
| Generating Visual Scenes from Touch | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fredfyyang.github.io/vision-from-touch/) <br /> [![GitHub](https://img.shields.io/github/stars/fredfyyang/vision-from-touch?style=flat)](https://github.com/fredfyyang/vision-from-touch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15117-b31b1b.svg)](https://arxiv.org/abs/2309.15117) | :heavy_minus_sign: |
| Multimodal High-Order Relation Transformer for Scene Boundary Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Multimodal_High-order_Relation_Transformer_for_Scene_Boundary_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Muscles in Action | [![GitHub](https://img.shields.io/github/stars/mchiquier/musclesinaction?style=flat)](https://github.com/mchiquier/musclesinaction) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chiquier_Muscles_in_Action_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02978-b31b1b.svg)](https://arxiv.org/abs/2212.02978) | :heavy_minus_sign: |
| Self-Evolved Dynamic Expansion Model for Task-Free Continual Learning | [![GitHub](https://img.shields.io/github/stars/dtuzi123/SEDEM?style=flat)](https://github.com/dtuzi123/SEDEM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Self-Evolved_Dynamic_Expansion_Model_for_Task-Free_Continual_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Multi-Event Video-Text Retrieval | [![GitHub](https://img.shields.io/github/stars/gengyuanmax/MeVTR?style=flat)](https://github.com/gengyuanmax/MeVTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Multi-Event_Video-Text_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11551-b31b1b.svg)](https://arxiv.org/abs/2308.11551) | :heavy_minus_sign: |
| Referring Image Segmentation using Text Supervision | [![GitHub](https://img.shields.io/github/stars/fawnliu/TRIS?style=flat)](https://github.com/fawnliu/TRIS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Referring_Image_Segmentation_Using_Text_Supervision_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14575-b31b1b.svg)](https://arxiv.org/abs/2308.14575) | :heavy_minus_sign: |
| Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning | [![GitHub](https://img.shields.io/github/stars/NMS05/Audio-Visual-Deception-Detection-DOLOS-Dataset-and-Parameter-Efficient-Crossmodal-Learning?style=flat)](https://github.com/NMS05/Audio-Visual-Deception-Detection-DOLOS-Dataset-and-Parameter-Efficient-Crossmodal-Learning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Audio-Visual_Deception_Detection_DOLOS_Dataset_and_Parameter-Efficient_Crossmodal_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12745-b31b1b.svg)](https://arxiv.org/abs/2303.12745) | :heavy_minus_sign: |
| EMMN: Emotional Motion Memory Network for Audio-Driven Emotional Talking Face Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tan_EMMN_Emotional_Motion_Memory_Network_for_Audio-driven_Emotional_Talking_Face_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-Training | [![GitHub](https://img.shields.io/github/stars/tyhuang0428/CLIP2Point?style=flat)](https://github.com/tyhuang0428/CLIP2Point) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_CLIP2Point_Transfer_CLIP_to_Point_Cloud_Classification_with_Image-Depth_Pre-Training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.01055-b31b1b.svg)](https://arxiv.org/abs/2210.01055) | :heavy_minus_sign: |
| Speech2Lip: High-Fidelity Speech to Lip Generation by Learning from a Short Video | [![GitHub](https://img.shields.io/github/stars/CVMI-Lab/Speech2Lip?style=flat)](https://github.com/CVMI-Lab/Speech2Lip) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Speech2Lip_High-fidelity_Speech_to_Lip_Generation_by_Learning_from_a_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.04814-b31b1b.svg)](https://arxiv.org/abs/2309.04814) | :heavy_minus_sign: |
| GrowCLIP: Data-Aware Automatic Model Growing for Large-Scale Contrastive Language-Image Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_GrowCLIP_Data-Aware_Automatic_Model_Growing_for_Large-scale_Contrastive_Language-Image_Pre-Training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11331-b31b1b.svg)](https://arxiv.org/abs/2308.11331) | :heavy_minus_sign: |
| A Retrospect to Multi-Prompt Learning Across Vision and Language | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_A_Retrospect_to_Multi-prompt_Learning_across_Vision_and_Language_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules | [![GitHub](https://img.shields.io/github/stars/zhiqic/ChartReader?style=flat)](https://github.com/zhiqic/ChartReader) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_ChartReader_A_Unified_Framework_for_Chart_Derendering_and_Comprehension_without_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02173-b31b1b.svg)](https://arxiv.org/abs/2304.02173) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VIOGL4gF06w) |
| Boosting Multi-Modal Model Performance with Adaptive Gradient Modulation | [![GitHub](https://img.shields.io/github/stars/lihong2303/AGM_ICCV2023?style=flat)](https://github.com/lihong2303/AGM_ICCV2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Boosting_Multi-modal_Model_Performance_with_Adaptive_Gradient_Modulation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07686-b31b1b.svg)](https://arxiv.org/abs/2308.07686) | :heavy_minus_sign: |
| ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data | [![GitHub](https://img.shields.io/github/stars/StanfordMIMI/villa?style=flat)](https://github.com/StanfordMIMI/villa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Varma_ViLLA_Fine-Grained_Vision-Language_Representation_Learning_from_Real-World_Data_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11194-b31b1b.svg)](https://arxiv.org/abs/2308.11194) | :heavy_minus_sign: |
| Robust Referring Video Object Segmentation with Cyclic Structural Consensus | [![GitHub](https://img.shields.io/github/stars/lxa9867/R2VOS?style=flat)](https://github.com/lxa9867/R2VOS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Robust_Referring_Video_Object_Segmentation_with_Cyclic_Structural_Consensus_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.01203-b31b1b.svg)](https://arxiv.org/abs/2207.01203) | :heavy_minus_sign: |
| Fantasia3D: Disentangling Geometry and Appearance for High-Quality Text-to-3D Content Creation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fantasia3d.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Gorilla-Lab-SCUT/Fantasia3D?style=flat)](https://github.com/Gorilla-Lab-SCUT/Fantasia3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Fantasia3D_Disentangling_Geometry_and_Appearance_for_High-quality_Text-to-3D_Content_Creation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13873-b31b1b.svg)](https://arxiv.org/abs/2303.13873) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Xbzl4HzFiNo) |
| CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation | [![GitHub](https://img.shields.io/github/stars/KevinLight831/CTP?style=flat)](https://github.com/KevinLight831/CTP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_CTPTowards_Vision-Language_Continual_Pretraining_via_Compatible_Momentum_Contrast_and_Topology_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07146-b31b1b.svg)](https://arxiv.org/abs/2308.07146) | :heavy_minus_sign: |
| Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://haibiaoxuan.github.io/Narrator/) <br /> [![GitHub](https://img.shields.io/github/stars/HaibiaoXuan/Narrator?style=flat)](https://github.com/HaibiaoXuan/Narrator) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xuan_Narrator_Towards_Natural_Control_of_Human-Scene_Interaction_Generation_via_Relationship_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09410-b31b1b.svg)](https://arxiv.org/abs/2303.09410) | :heavy_minus_sign: |
