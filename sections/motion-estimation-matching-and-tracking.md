# ICCV-2023-Papers

## Motion Estimation, Matching and Tracking

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| TMR: Text-to-Motion Retrieval using Contrastive 3D Human Motion Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mathis.petrovich.fr/tmr/) <br /> [![GitHub](https://img.shields.io/github/stars/Mathux/TMR)](https://github.com/Mathux/TMR) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Demo-FFD21F.svg)](https://huggingface.co/spaces/Mathux/TMR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Petrovich_TMR_Text-to-Motion_Retrieval_Using_Contrastive_3D_Human_Motion_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00976-b31b1b.svg)](https://arxiv.org/abs/2305.00976) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FK0RukgDEtM) |
| Sequential Texts Driven Cohesive Motions Synthesis with Natural Transitions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://druthrie.github.io/sequential-texts-to-motion/) <br /> [![GitHub](https://img.shields.io/github/stars/Druthrie/ST2M)](https://github.com/Druthrie/ST2M) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Sequential_Texts_Driven_Cohesive_Motions_Synthesis_with_Natural_Transitions_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Auxiliary Tasks Benefit 3D Skeleton-based Human Motion Prediction | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/AuxFormer)](https://github.com/MediaBrain-SJTU/AuxFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Auxiliary_Tasks_Benefit_3D_Skeleton-based_Human_Motion_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08942-b31b1b.svg)](https://arxiv.org/abs/2308.08942) | :heavy_minus_sign: |
| Explicit Motion Disentangling for Efficient Optical Flow Estimation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_Explicit_Motion_Disentangling_for_Efficient_Optical_Flow_Estimation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TrackFlow: Multi-Object tracking with Normalizing Flows | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mancusi_TrackFlow_Multi-Object_tracking_with_Normalizing_Flows_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11513-b31b1b.svg)](https://arxiv.org/abs/2308.11513) | :heavy_minus_sign: |
| HumanMAC: Masked Motion Completion for Human Motion Prediction | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://lhchen.top/Human-MAC/) <br /> [![GitHub](https://img.shields.io/github/stars/LinghaoChan/HumanMAC)](https://github.com/LinghaoChan/HumanMAC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_HumanMAC_Masked_Motion_Completion_for_Human_Motion_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.03665-b31b1b.svg)](https://arxiv.org/abs/2302.03665) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vfde9GdUHBs) |
| Geometrized Transformer for Self-Supervised Homography Estimation | [![GitHub](https://img.shields.io/github/stars/ruc-aimc-lab/GeoFormer)](https://github.com/ruc-aimc-lab/GeoFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Geometrized_Transformer_for_Self-Supervised_Homography_Estimation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving | [![GitHub](https://img.shields.io/github/stars/duke-vision/semantic-unsup-flow-release)](https://github.com/duke-vision/semantic-unsup-flow-release) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_SemARFlow_Injecting_Semantics_into_Unsupervised_Optical_Flow_Estimation_for_Autonomous_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06209-b31b1b.svg)](https://arxiv.org/abs/2303.06209) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XYBTolH2S8A) |
| NeSS-ST: Detecting Good and Stable Keypoints with a Neural Stability Score and the Shi-Tomasi Detector | [![GitHub](https://img.shields.io/github/stars/KonstantinPakulev/NeSS-ST)](https://github.com/KonstantinPakulev/NeSS-ST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pakulev_NeSS-ST_Detecting_Good_and_Stable_Keypoints_with_a_Neural_Stability_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Robust Object Modeling for Visual Tracking | [![GitHub](https://img.shields.io/github/stars/dawnyc/ROMTrack)](https://github.com/dawnyc/ROMTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_Robust_Object_Modeling_for_Visual_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05140-b31b1b.svg)](https://arxiv.org/abs/2308.05140) | :heavy_minus_sign: |
| Social Diffusion: Long-Term Multiple Human Motion Anticipation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tanke_Social_Diffusion_Long-term_Multiple_Human_Motion_Anticipation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking | [![GitHub](https://img.shields.io/github/stars/kangben258/HiT)](https://github.com/kangben258/HiT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Exploring_Lightweight_Hierarchical_Vision_Transformers_for_Efficient_Visual_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06904-b31b1b.svg)](https://arxiv.org/abs/2308.06904) | :heavy_minus_sign: |
| HMD-NeMo: Online 3D Avatar Motion Generation from Sparse Observations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Aliakbarian_HMD-NeMo_Online_3D_Avatar_Motion_Generation_From_Sparse_Observations_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11261-b31b1b.svg)](https://arxiv.org/abs/2308.11261) | :heavy_minus_sign: |
| Learning Fine-Grained Features for Pixel-Wise Video Correspondences | [![GitHub](https://img.shields.io/github/stars/qianduoduolr/Spa-then-Temp)](https://github.com/qianduoduolr/Spa-then-Temp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Learning_Fine-Grained_Features_for_Pixel-Wise_Video_Correspondences_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03040-b31b1b.svg)](https://arxiv.org/abs/2308.03040) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2ZCVUoiyM0U) |
| GAFlow: Incorporating Gaussian Attention into Optical Flow | [![GitHub](https://img.shields.io/github/stars/LA30/GAFlow)](https://github.com/LA30/GAFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_GAFlow_Incorporating_Gaussian_Attention_into_Optical_Flow_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Occ<sup>2</sup>Net: Robust Image Matching based on 3D Occupancy Estimation for Occluded Regions | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Occ2Net_Robust_Image_Matching_Based_on_3D_Occupancy_Estimation_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16160-b31b1b.svg)](https://arxiv.org/abs/2308.16160) | :heavy_minus_sign: |
| Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiyewise.github.io/projects/LAMA/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02667-b31b1b.svg)](https://arxiv.org/abs/2301.02667) | :heavy_minus_sign: |
| Trajectory Unified Transformer for Pedestrian Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/lssiair/TUTR)](https://github.com/lssiair/TUTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Trajectory_Unified_Transformer_for_Pedestrian_Trajectory_Prediction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TMA: Temporal Motion Aggregation for Event-based Optical Flow | [![GitHub](https://img.shields.io/github/stars/ispc-lab/TMA)](https://github.com/ispc-lab/TMA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_TMA_Temporal_Motion_Aggregation_for_Event-based_Optical_Flow_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11629-b31b1b.svg)](https://arxiv.org/abs/2303.11629) | :heavy_minus_sign: |
| Taming Contrast Maximization for Learning Sequential, Low-Latency, Event-based Optical Flow | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mavlab.tudelft.nl/taming_event_flow/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Paredes-Valles_Taming_Contrast_Maximization_for_Learning_Sequential_Low-latency_Event-based_Optical_Flow_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vkYimENc494) |
| GlueStick: Robust Image Matching by Sticking Points and Lines Together | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://iago-suarez.com/gluestick/) <br /> [![GitHub](https://img.shields.io/github/stars/cvg/GlueStick)](https://github.com/cvg/GlueStick) <br /> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cvg/GlueStick/blob/main/gluestick_matching_demo.ipynb) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pautrat_GlueStick_Robust_Image_Matching_by_Sticking_Points_and_Lines_Together_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02008-b31b1b.svg)](https://arxiv.org/abs/2304.02008) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JmpddJ5pfz8) |
| DARTH: Holistic Test-Time Adaptation for Multiple Object Tracking | [![GitHub](https://img.shields.io/github/stars/mattiasegu/darth)](https://github.com/mattiasegu/darth) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Segu_DARTH_Holistic_Test-time_Adaptation_for_Multiple_Object_Tracking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| S-TREK: Sequential Translation and Rotation Equivariant Keypoints for Local Feature Extraction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Santellani_S-TREK_Sequential_Translation_and_Rotation_Equivariant_Keypoints_for_Local_Feature_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14598-b31b1b.svg)](https://arxiv.org/abs/2308.14598) | :heavy_minus_sign: |
| Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation | [![GitHub](https://img.shields.io/github/stars/yoxu515/MITS)](https://github.com/yoxu515/MITS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Integrating_Boxes_and_Masks_A_Multi-Object_Framework_for_Unified_Visual_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13266-b31b1b.svg)](https://arxiv.org/abs/2308.13266) | :heavy_minus_sign: |
| Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://fabiendelattre.com/robust-rotation-estimation/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Delattre_Robust_Frame-to-Frame_Camera_Rotation_Estimation_in_Crowded_Scenes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.08588-b31b1b.svg)](https://arxiv.org/abs/2309.08588) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SL4QBedLu9Q) |
| Sparse Instance Conditioned Multimodal Trajectory Prediction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Sparse_Instance_Conditioned_Multimodal_Trajectory_Prediction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://posediffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/PoseDiffusion)](https://github.com/facebookresearch/PoseDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_PoseDiffusion_Solving_Pose_Estimation_via_Diffusion-aided_Bundle_Adjustment_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.15667-b31b1b.svg)](https://arxiv.org/abs/2306.15667) | :heavy_minus_sign: |
| 3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/dsx0511/3DMOTFormer)](https://github.com/dsx0511/3DMOTFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_3DMOTFormer_Graph_Transformer_for_Online_3D_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06635-b31b1b.svg)](https://arxiv.org/abs/2308.06635) | :heavy_minus_sign: |
| Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/meaten/FlowChain-ICCV2023)](https://github.com/meaten/FlowChain-ICCV2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Maeda_Fast_Inference_and_Update_of_Probabilistic_Density_Estimation_on_Trajectory_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08824-b31b1b.svg)](https://arxiv.org/abs/2308.08824) | :heavy_minus_sign: |
| Supervised Homography Learning with Realistic Dataset Generation | [![GitHub](https://img.shields.io/github/stars/JianghaiSCU/RealSH)](https://github.com/JianghaiSCU/RealSH) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Supervised_Homography_Learning_with_Realistic_Dataset_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15353-b31b1b.svg)](https://arxiv.org/abs/2307.15353) | :heavy_minus_sign: |
| Joint-Relation Transformer for Multi-Person Motion Prediction | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/JRTransformer)](https://github.com/MediaBrain-SJTU/JRTransformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Joint-Relation_Transformer_for_Multi-Person_Motion_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04808-b31b1b.svg)](https://arxiv.org/abs/2308.04808) | :heavy_minus_sign: |
| Event-based Temporally Dense Optical Flow Estimation with Sequential Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ponghiran_Event-based_Temporally_Dense_Optical_Flow_Estimation_with_Sequential_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.01244-b31b1b.svg)](https://arxiv.org/abs/2210.01244) | :heavy_minus_sign: |
| 3D Motion Magnification: Visualizing Subtle Motions from Time-Varying Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3d-motion-magnification.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/3d-motion-magnification/3d-motion-mag)](https://github.com/3d-motion-magnification/3d-motion-mag) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_3D_Motion_Magnification_Visualizing_Subtle_Motions_from_Time-Varying_Radiance_Fields_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03757-b31b1b.svg)](https://arxiv.org/abs/2308.03757) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ljar4GAFkUk&t=146s) |
| Learning Optical Flow from Event Camera with Rendered Dataset | [![GitHub](https://img.shields.io/github/stars/boomluo02/ADMFlow)](https://github.com/boomluo02/ADMFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Learning_Optical_Flow_from_Event_Camera_with_Rendered_Dataset_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11011-b31b1b.svg)](https://arxiv.org/abs/2303.11011) | :heavy_minus_sign: |
| Persistent-Transient Duality: A Multi-Mechanism Approach for Modeling Human-Object Interaction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tran_Persistent-Transient_Duality_A_Multi-Mechanism_Approach_for_Modeling_Human-Object_Interaction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12729-b31b1b.svg)](https://arxiv.org/abs/2307.12729) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nVOQdI8g7AY) |
| Deep Homography Mixture for Single Image Rolling Shutter Correction | [![GitHub](https://img.shields.io/github/stars/DavidYan2001/Deep_RS-HM)](https://github.com/DavidYan2001/Deep_RS-HM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Deep_Homography_Mixture_for_Single_Image_Rolling_Shutter_Correction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Fast Neural Scene Flow | [![GitHub](https://img.shields.io/github/stars/Lilac-Lee/FastNSF)](https://github.com/Lilac-Lee/FastNSF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Fast_Neural_Scene_Flow_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.09121-b31b1b.svg)](https://arxiv.org/abs/2304.09121) | :heavy_minus_sign: |
| RLSAC: Reinforcement Learning Enhanced Sample Consensus for End-to-End Robust Estimation | [![GitHub](https://img.shields.io/github/stars/IRMVLab/RLSAC)](https://github.com/IRMVLab/RLSAC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nie_RLSAC_Reinforcement_Learning_Enhanced_Sample_Consensus_for_End-to-End_Robust_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05318-b31b1b.svg)](https://arxiv.org/abs/2308.05318) | :heavy_minus_sign: |
| MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/MeMOTR)](https://github.com/MCG-NJU/MeMOTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_MeMOTR_Long-Term_Memory-Augmented_Transformer_for_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15700-b31b1b.svg)](https://arxiv.org/abs/2307.15700) | :heavy_minus_sign: |
| MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors | [![GitHub](https://img.shields.io/github/stars/slothfulxtx/MBPTrack3D)](https://github.com/slothfulxtx/MBPTrack3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_MBPTrack_Improving_3D_Point_Cloud_Tracking_with_Memory_Networks_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05071-b31b1b.svg)](https://arxiv.org/abs/2303.05071) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Zl_4LnoX_Ak) |
| SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/SportsMOT)](https://github.com/MCG-NJU/SportsMOT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cui_SportsMOT_A_Large_Multi-Object_Tracking_Dataset_in_Multiple_Sports_Scenes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05170-b31b1b.svg)](https://arxiv.org/abs/2304.05170) | :heavy_minus_sign: |
| Heterogeneous Diversity Driven Active Learning for Multi-Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Heterogeneous_Diversity_Driven_Active_Learning_for_Multi-Object_Tracking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://garfield-kh.github.io/TM2D/) <br /> [![GitHub](https://img.shields.io/github/stars/Garfield-kh/TM2D)](https://github.com/Garfield-kh/TM2D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_TM2D_Bimodality_Driven_3D_Dance_Generation_via_Music-Text_Integration_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02419-b31b1b.svg)](https://arxiv.org/abs/2304.02419) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6QQFXG4s7iQ) |
| Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Synchronize_Feature_Extracting_and_Matching_A_Single_Branch_Framework_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12549-b31b1b.svg)](https://arxiv.org/abs/2308.12549) | :heavy_minus_sign: |
| Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/yolomax/ColTrack)](https://github.com/yolomax/ColTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Collaborative_Tracking_Learning_for_Frame-Rate-Insensitive_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05911-b31b1b.svg)](https://arxiv.org/abs/2308.05911) | :heavy_minus_sign: |
| CiteTracker: Correlating Image and Text for Visual Tracking | [![GitHub](https://img.shields.io/github/stars/NorahGreen/CiteTracker)](https://github.com/NorahGreen/CiteTracker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_CiteTracker_Correlating_Image_and_Text_for_Visual_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11322-b31b1b.svg)](https://arxiv.org/abs/2308.11322) | :heavy_minus_sign: |
| SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sinc.is.tue.mpg.de/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Athanasiou_SINC_Spatial_Composition_of_3D_Human_Motions_for_Simultaneous_Action_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10417-b31b1b.svg)](https://arxiv.org/abs/2304.10417) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uwUriDnKTLI) |
| Uncertainty-Aware Unsupervised Multi-Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Uncertainty-aware_Unsupervised_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15409-b31b1b.svg)](https://arxiv.org/abs/2307.15409) | :heavy_minus_sign: |
| PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jaraxxus-me.github.io/ICCV2023_PVTpp/) <br /> [![GitHub](https://img.shields.io/github/stars/Jaraxxus-Me/PVT_pp)](https://github.com/Jaraxxus-Me/PVT_pp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_PVT_A_Simple_End-to-End_Latency-Aware_Visual_Tracking_Framework_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ihbae.com/publication/eigentrajectory/) <br /> [![GitHub](https://img.shields.io/github/stars/inhwanbae/EigenTrajectory)](https://github.com/inhwanbae/EigenTrajectory) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bae_EigenTrajectory_Low-Rank_Descriptors_for_Multi-Modal_Trajectory_Forecasting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09306-b31b1b.svg)](https://arxiv.org/abs/2307.09306) | :heavy_minus_sign: |
| RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://npucvr.github.io/RPEFlow/) <br /> [![GitHub](https://img.shields.io/github/stars/danqu130/RPEFlow)](https://github.com/danqu130/RPEFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wan_RPEFlow_Multimodal_Fusion_of_RGB-PointCloud-Event_for_Joint_Optical_Flow_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15082-b31b1b.svg)](https://arxiv.org/abs/2309.15082) | :heavy_minus_sign: |
| Multi-Scale Bidirectional Recurrent Network with Hybrid Correlation for Point Cloud based Scene Flow Estimation | [![GitHub](https://img.shields.io/github/stars/cwc1260/MSBRN)](https://github.com/cwc1260/MSBRN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Multi-Scale_Bidirectional_Recurrent_Network_with_Hybrid_Correlation_for_Point_Cloud_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/chengche6230/ReST)](https://github.com/chengche6230/ReST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_ReST_A_Reconfigurable_Spatial-Temporal_Graph_Model_for_Multi-Camera_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13229-b31b1b.svg)](https://arxiv.org/abs/2308.13229) | :heavy_minus_sign: |
| TAPIR: Tracking any Point with Per-Frame Initialization and Temporal Refinement | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://deepmind-tapir.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/google-deepmind/tapnet)](https://github.com/google-deepmind/tapnet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08637-b31b1b.svg)](https://arxiv.org/abs/2306.08637) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=I1DQJH3v7Nk) |
| IHNet: Iterative Hierarchical Network Guided by High-Resolution Estimated Information for Scene Flow Estimation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_IHNet_Iterative_Hierarchical_Network_Guided_by_High-Resolution_Estimated_Information_for_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Can Language Models Learn to Listen? | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/) <br /> [![GitHub](https://img.shields.io/github/stars/sanjayss34/lm-listener)](https://github.com/sanjayss34/lm-listener) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ng_Can_Language_Models_Learn_to_Listen_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10897-b31b1b.svg)](https://arxiv.org/abs/2308.10897) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=djpSOhdIU8M) |
| XVO: Generalized Visual Odometry via Cross-Modal Self-Training | [![GitHub](https://img.shields.io/github/stars/h2xlab/XVO)](https://github.com/h2xlab/XVO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_XVO_Generalized_Visual_Odometry_via_Cross-Modal_Self-Training_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Distracting Downpour: Adversarial Weather Attacks for Motion Estimation | [![GitHub](https://img.shields.io/github/stars/cv-stuttgart/DistractingDownpour)](https://github.com/cv-stuttgart/DistractingDownpour) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Schmalfuss_Distracting_Downpour_Adversarial_Weather_Attacks_for_Motion_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.06716-b31b1b.svg)](https://arxiv.org/abs/2305.06716) | :heavy_minus_sign: |
| Foreground-Background Distribution Modeling Transformer for Visual Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Foreground-Background_Distribution_Modeling_Transformer_for_Visual_Object_Tracking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
