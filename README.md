# ICCV-2023-Papers

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
![Version](https://img.shields.io/badge/version-v0.0.0-rc0)
![GitHub repo size](https://img.shields.io/github/repo-size/DmitryRyumin/ICCV-2023-Papers)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/LICENSE)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/README.md)
![GitHub contributors](https://img.shields.io/github/contributors/dmitryryumin/ICCV-2023-Papers)
![GitHub commit activity (branch)](https://img.shields.io/github/commit-activity/t/dmitryryumin/ICCV-2023-Papers)
![GitHub closed issues](https://img.shields.io/github/issues-closed/DmitryRyumin/ICCV-2023-Papers)
![GitHub issues](https://img.shields.io/github/issues/DmitryRyumin/ICCV-2023-Papers)
![GitHub closed pull requests](https://img.shields.io/github/issues-pr-closed/DmitryRyumin/ICCV-2023-Papers)
![GitHub pull requests](https://img.shields.io/github/issues-pr/dmitryryumin/ICCV-2023-Papers)
![GitHub last commit](https://img.shields.io/github/last-commit/DmitryRyumin/ICCV-2023-Papers)
![GitHub watchers](https://img.shields.io/github/watchers/dmitryryumin/ICCV-2023-Papers)
![GitHub forks](https://img.shields.io/github/forks/dmitryryumin/ICCV-2023-Papers)
![GitHub Repo stars](https://img.shields.io/github/stars/dmitryryumin/ICCV-2023-Papers)
![Visitors](https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fgithub.com%2FDmitryRyumin%2FICCV-2023-Papers&label=Visitors&countColor=%23263759&style=flat)

---

ICCV 2023 Papers: Explore a comprehensive collection of cutting-edge research papers presented at [*ICCV 2023*](https://iccv2023.thecvf.com/), the premier computer vision conference. Keep up to date with the latest advances in computer vision and deep learning. Code implementations included. :star: the repository for the development of visual intelligence!

<p align="center">
    <a href="https://iccv2023.thecvf.com/" target="_blank">
        <img width="600" src="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/images/ICCV2023-banner.jpg" alt="ICCV 2023">
    </a>
<p>

---

[*The online version of the ICCV 2023 Conference Programme*](https://iccv2023.thecvf.com/main.conference.program-107.php), comprises a list of all accepted full papers, their presentation order, as well as the designated presentation times.

---

<a href="https://github.com/DmitryRyumin/NewEraAI-Papers" style="float:left;">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/arrow_click_cursor_pointer.png" width="25" />
  Other collections of the best AI conferences
</a>

<br />
<br />

> :exclamation: Conference table will be up to date all the time.

<table>
    <tr>
        <td><strong>Conference</strong></td>
        <td colspan="1" align="center"><strong>Year</strong></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Computer Vision (CV)</i></td>
    </tr>
    <tr>
        <td>CVPR</td>
        <td><a href="https://github.com/DmitryRyumin/CVPR-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Speech (SP)</i></td>
    </tr>
    <tr>
        <td>ICASSP</td>
        <td><a href="https://github.com/DmitryRyumin/ICASSP-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
        <td>INTERSPEECH</td>
        <td><a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers" target="_blank">2023</a></td>
    </tr>
</table>

---

## Contributors

<a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/graphs/contributors">
  <img src="http://contributors.nn.ci/api?repo=DmitryRyumin/ICCV-2023-Papers" />
</a>

<br />
<br />

Contributions to improve the completeness of this list are greatly appreciated. If you come across any overlooked papers, please **feel free to [*create pull requests*](https://github.com/DmitryRyumin/ICCV-2023-Papers/pulls), [*open issues*](https://github.com/DmitryRyumin/ICCV-2023-Papers/issues) or contact me via [*email*](mailto:neweraairesearch@gmail.com)**. Your participation is crucial to making this repository even better.

---

## [Papers](https://iccv2023.thecvf.com/main.conference.program-107.php)

> :exclamation: Final paper links will be added post-conference.

<details open>
<summary>List of sections<a id="sections"></a></summary>

- [3D from Multi-View and Sensors](#3d-from-multi-view-and-sensors)
- [Adversarial Attack and Defense](#adversarial-attack-and-defense)
- [Vision and Robotics](#vision-and-robotics)
- [Vision and Graphics](#vision-and-graphics)
- [Segmentation, Grouping and Shape Analysis](#segmentation-grouping-and-shape-analysis)
- [Recognition: Categorization](#recognition-categorization)
- [Explainable AI for CV](#explainable-ai-for-cv)
- [Neural Generative Models](#neural-generative-models)
- [Vision and Language](#vision-and-language)
- [Vision, Graphics, and Robotics](#vision-graphics-and-robotics)
- [Privacy, Security, Fairness, and Explainability](#privacy-security-fairness-and-explainability)
- [Fairness, Privacy, Ethics, Social-good, Transparency, Accountability in Vision](#fairness-privacy-ethics-social-good-transparency-accountability-in-vision)
- [First Person (Egocentric) Vision](#first-person-egocentric-vision)
- [Deep Learning Architectures](#deep-learning-architectures)
- [Recognition: Detection](#recognition-detection)
- [Image and Video Synthesis](#image-and-video-synthesis)
- [Vision and Audio](#vision-and-audio)
- [Recognition, Segmentation, and Shape Analysis](#recognition-segmentation-and-shape-analysis)
- [Generative AI](#generative-ai)
- [Humans, 3D Modeling, and Driving](#humans-3d-modeling-and-driving)
- [Low-Level Vision and Theory](#low-level-vision-and-theory)
- [Navigation and Autonomous Driving](#navigation-and-autonomous-driving)
- [3D from a Single Image and Shape-from-X](#3d-from-a-single-image-and-shape-from-x)
- [Motion Estimation, Matching and Tracking](#motion-estimation-matching-and-tracking)
- [Action and Event Understanding](#action-and-event-understanding)
- [Computational Imaging](#computational-imaging)
- [Embodied Vision: Active Agents; Simulation](#embodied-vision-active-agents-simulation)
- [Recognition: Retrieval](#recognition-retrieval)
- [Transfer, Low-Shot, Continual, Long-Tail Learning](#transfer-low-shot-continual-long-tail-learning)
- [Low-Level and Physics-based Vision](#low-level-and-physics-based-vision)
- [Computer Vision Theory](#computer-vision-theory)
- [Video Analysis and Understanding](#video-analysis-and-understanding)
- [Object Pose Estimation and Tracking](#object-pose-estimation-and-tracking)
- [3D Shape Modeling and Processing](#3d-shape-modeling-and-processing)
- [Human Pose/Shape Estimation](#human-poseshape-estimation)
- [Transfer, Low-Shot, and Continual Learning](#transfer-low-shot-and-continual-learning)
- [Self-, Semi-, and Unsupervised Learning](#self--semi--and-unsupervised-learning)
- [Self-, Semi-, Meta-, Unsupervised Learning](#self--semi--meta--unsupervised-learning)
- [Photogrammetry and Remote Sensing](#photogrammetry-and-remote-sensing)
- [Efficient and Scalable Vision](#efficient-and-scalable-vision)
- [Machine Learning (other than Deep Learning)](#machine-learning-other-than-deep-learning)
- [Document Analysis and Understanding](#document-analysis-and-understanding)
- [Biometrics](#biometrics)
- [Datasets and Evaluation](#datasets-and-evaluation)
- [Faces and Gestures](#faces-and-gestures)
- [Medical and Biological Vision; Cell Microscopy](#medical-and-biological-vision-cell-microscopy)
- [Scene Analysis and Understanding](#scene-analysis-and-understanding)
- [Multimodal Learning](#multimodal-learning)
- [Human-in-the-Loop Computer Vision](#human-in-the-loop-computer-vision)
- [Image and Video Forensics](#image-and-video-forensics)
- [Geometric Deep Learning](#geometric-deep-learning)
- [Vision Applications and Systems](#vision-applications-and-systems)
- [Machine Learning and Dataset](#machine-learning-and-dataset)

</details>

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D from Multi-View and Sensors

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.14383-b31b1b.svg)](https://arxiv.org/abs/2308.14383) | :heavy_minus_sign: |
| ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cy94.github.io/scannetpp/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11417-b31b1b.svg)](https://arxiv.org/abs/2308.11417) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=E6P9e2r6M8I) |
| Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Doppelgangers: Learning to Disambiguate Images of Similar Structures | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://doppelgangers-3d.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/RuojinCai/Doppelgangers)](https://github.com/RuojinCai/Doppelgangers) | :heavy_minus_sign: | :heavy_minus_sign: |
| EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries | [![GitHub](https://img.shields.io/github/stars/Wayne-Mai/EgoLoc)](https://github.com/Wayne-Mai/EgoLoc) | [![arXiv](https://img.shields.io/badge/arXiv-2212.06969-b31b1b.svg)](https://arxiv.org/abs/2212.06969) | :heavy_minus_sign: |
| ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via an Indirect Recording Solution | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nexuslrf.github.io/ENVIDR/) <br /> [![GitHub](https://img.shields.io/github/stars/nexuslrf/ENVIDR)](https://github.com/nexuslrf/ENVIDR) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13022-b31b1b.svg)](https://arxiv.org/abs/2303.13022) | [![Google Drive](https://img.shields.io/badge/Google%20Drive-4285F4?style=for-the-badge&logo=googledrive&logoColor=white)](https://drive.google.com/file/d/18kU-IWVxboCG8SCGgrBA5JHC0JIgPCS8/view?t=17s) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Adversarial Attack and Defense

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Robust Mixture-of-Expert Training for Convolutional Neural Networks | [![GitHub](https://img.shields.io/github/stars/OPTML-Group/Robust-MoE-CNN)](https://github.com/OPTML-Group/Robust-MoE-CNN) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10110-b31b1b.svg)](https://arxiv.org/abs/2308.10110) | :heavy_minus_sign: |
| Set-Level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-Training Models | [![GitHub](https://img.shields.io/github/stars/Zoky-2020/SGA)](https://github.com/Zoky-2020/SGA) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14061-b31b1b.svg)](https://arxiv.org/abs/2307.14061) | :heavy_minus_sign: |
| CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning | [![GitHub](https://img.shields.io/github/stars/nishadsinghi/CleanCLIP)](https://github.com/nishadsinghi/CleanCLIP) | [![arXiv](https://img.shields.io/badge/arXiv-2303.03323-b31b1b.svg)](https://arxiv.org/abs/2303.03323) | :heavy_minus_sign: |
| CGBA: Curvature-Aware Geometric Black-Box Attack | [![GitHub](https://img.shields.io/github/stars/Farhamdur/CGBA)](https://github.com/Farhamdur/CGBA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.03163-b31b1b.svg)](https://arxiv.org/abs/2308.03163) | :heavy_minus_sign: |
| Robust Evaluation of Diffusion-based Adversarial Purification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.09051-b31b1b.svg)](https://arxiv.org/abs/2303.09051) | :heavy_minus_sign: |
| Advancing Example Exploitation can Alleviate Critical Challenges in Adversarial Training | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| The Victim and the Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models | [![GitHub](https://img.shields.io/github/stars/SRI-CSL/TIJO)](https://github.com/SRI-CSL/TIJO) | [![arXiv](https://img.shields.io/badge/arXiv-2308.03906-b31b1b.svg)](https://arxiv.org/abs/2308.03906) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Robotics

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Simoun: Synergizing Interactive Motion-Appearance Understanding for Vision-based Reinforcement Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Among Us: Adversarially Robust Collaborative Perception by Consensus | [![GitHub](https://img.shields.io/github/stars/coperception/ROBOSAC)](https://github.com/coperception/ROBOSAC) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09495-b31b1b.svg)](https://arxiv.org/abs/2303.09495) | :heavy_minus_sign: |
| Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://saltoricristiano.github.io/lidog/) <br /> [![GitHub](https://img.shields.io/github/stars/saltoricristiano/LiDOG)](https://github.com/saltoricristiano/LiDOG) | [![arXiv](https://img.shields.io/badge/arXiv-2304.11705-b31b1b.svg)](https://arxiv.org/abs/2304.11705) | :heavy_minus_sign: |
| Stabilizing Visual Reinforcement Learning via Asymmetric Interactive Cooperation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MAAL: Multimodality-Aware Autoencoder-based Affordance Learning for 3D Articulated Objects | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Rethinking Range View Representation for LiDAR Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.05367-b31b1b.svg)](https://arxiv.org/abs/2303.05367) | :heavy_minus_sign: |
| PourIt!: Weakly-Supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hetolin.github.io/PourIt/) <br /> [![GitHub](https://img.shields.io/github/stars/hetolin/PourIt)](https://github.com/hetolin/PourIt) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11299-b31b1b.svg)](https://arxiv.org/abs/2307.11299) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=R5SpiV0658Q) |
| CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.04869-b31b1b.svg)](https://arxiv.org/abs/2303.04869) | :heavy_minus_sign: |
| Environment Agnostic Representation for Visual Reinforcement Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Test-Time Personalizable Forecasting of 3D Human Poses | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| HM-ViT: Hetero-Modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.10628-b31b1b.svg)](https://arxiv.org/abs/2304.10628) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Graphics

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Efficient Neural Supersampling on a Novel Gaming Dataset | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.01483-b31b1b.svg)](https://arxiv.org/abs/2308.01483) | :heavy_minus_sign: |
| Locally Stylized Neural Radiance Fields | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| NEMTO: Neural Environment Matting for Novel View and Relighting Synthesis of Transparent Objects | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.11963-b31b1b.svg)](https://arxiv.org/abs/2303.11963) | :heavy_minus_sign: |
| DDColor: Towards Photo-Realistic and Semantic-Aware Image Colorization via Dual Decoders | [![GitHub](https://img.shields.io/github/stars/piddnad/DDColor)](https://github.com/piddnad/DDColor) <br /> [![ModelScope](https://img.shields.io/badge/ModelScope-DDColor-614BFF.svg)](https://www.modelscope.cn/models/damo/cv_ddcolor_image-colorization/summary) | [![arXiv](https://img.shields.io/badge/arXiv-2212.11613-b31b1b.svg)](https://arxiv.org/abs/2212.11613) | :heavy_minus_sign: |
| IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zju3dv.github.io/intrinsic_nerf/) <br /> [![GitHub](https://img.shields.io/github/stars/zju3dv/IntrinsicNeRF)](https://github.com/zju3dv/IntrinsicNeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2210.00647-b31b1b.svg)](https://arxiv.org/abs/2210.00647) | :heavy_minus_sign: |
| PARIS: Part-Level Reconstruction and Motion Analysis for Articulated Objects | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3dlg-hcvc.github.io/paris/) <br /> [![GitHub](https://img.shields.io/github/stars/3dlg-hcvc/paris)](https://github.com/3dlg-hcvc/paris) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07391-b31b1b.svg)](https://arxiv.org/abs/2308.07391) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tDSrROPCgUc) |
| ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html) <br /> [![GitHub](https://img.shields.io/github/stars/mingyuan-zhang/ReMoDiffuse)](https://github.com/mingyuan-zhang/ReMoDiffuse) | [![arXiv](https://img.shields.io/badge/arXiv-2304.01116-b31b1b.svg)](https://arxiv.org/abs/2304.01116) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wSddrIA_2p8) |
| DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ds-fusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/tmaham/DS-Fusion)](https://github.com/tmaham/DS-Fusion) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-tmaham-FFD21F.svg)](https://huggingface.co/spaces/tmaham/DS-Fusion-Express) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09604-b31b1b.svg)](https://arxiv.org/abs/2303.09604) | :heavy_minus_sign: |
| Dynamic Mesh-Aware Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mesh-aware-rf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/YilingQiao/DMRF)](https://github.com/YilingQiao/DMRF) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1uXg76v0CNVxgrQfBHPR5SbxIMXyPLFfQ/view) | :heavy_minus_sign: |
| Neural Reconstruction of Relightable Human Model from Monocular Video | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Neural Microfacet Fields for Inverse Rendering | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://half-potato.gitlab.io/posts/nmf/) <br /> [![GitHub](https://img.shields.io/github/stars/half-potato/nmf)](https://github.com/half-potato/nmf) | [![arXiv](https://img.shields.io/badge/arXiv-2303.17806-b31b1b.svg)](https://arxiv.org/abs/2303.17806) | :heavy_minus_sign: |
| A Theory of Topological Derivatives for Inverse Rendering of Geometry | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ishit.github.io/td/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09865-b31b1b.svg)](https://arxiv.org/abs/2308.09865) | :heavy_minus_sign: |
| Vox-E: Text-Guided Voxel Editing of 3D Objects | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tau-vailab.github.io/Vox-E/) <br /> [![GitHub](https://img.shields.io/github/stars/TAU-VAILab/Vox-E)](https://github.com/TAU-VAILab/Vox-E) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12048-b31b1b.svg)](https://arxiv.org/abs/2303.12048) | :heavy_minus_sign: |
| StegaNeRF: Embedding Invisible Information within Neural Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xggnet.github.io/StegaNeRF/) <br /> [![GitHub](https://img.shields.io/github/stars/XGGNet/StegaNeRF)](https://github.com/XGGNet/StegaNeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2212.01602-b31b1b.svg)](https://arxiv.org/abs/2212.01602) | :heavy_minus_sign: |
| GlobalMapper: Arbitrary-Shaped Urban Layout Generation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.09693-b31b1b.svg)](https://arxiv.org/abs/2307.09693) | :heavy_minus_sign: |
| Urban Radiance Field Representation with Deformable Neural Mesh Primitives | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dnmp.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/DNMP/DNMP)](https://github.com/DNMP/DNMP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.10776-b31b1b.svg)](https://arxiv.org/abs/2307.10776) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JABhlaVq4VA) |
| End2End Multi-View Feature Matching with Differentiable Pose Optimization | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://barbararoessle.github.io/e2e_multi_view_matching/) | [![arXiv](https://img.shields.io/badge/arXiv-2205.01694-b31b1b.svg)](https://arxiv.org/abs/2205.01694) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5bFIIDOHRZY) |
| Tree-Structured Shading Decomposition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chen-geng.com/inv-shade-trees/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/gcgeng/inv-shade-trees)](https://github.com/gcgeng/inv-shade-trees) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://chen-geng.com/files/inv-shade-trees.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=L7zD9zM_zcg) |
| Lens Parameter Estimation for Realistic Depth of Field Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lvsn.github.io/inversedof/) | :heavy_minus_sign: | :heavy_minus_sign: |
| AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Cross-Modal Latent Space Alignment for Image to Avatar Translation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Computationally Efficient Neural Image Compression with Shallow Decoders | [![GitHub](https://img.shields.io/github/stars/mandt-lab/shallow-ntc)](https://github.com/mandt-lab/shallow-ntc) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06244-b31b1b.svg)](https://arxiv.org/abs/2304.06244) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Segmentation, Grouping and Shape Analysis

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Enhancing Spatial and Semantic Supervision for Hybrid-based 3D Instance Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Neural Eigenfunctions for Unsupervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/thudzj/NeuralEigenfunctionSegmentor)](https://github.com/thudzj/NeuralEigenfunctionSegmentor) | [![arXiv](https://img.shields.io/badge/arXiv-2304.02841-b31b1b.svg)](https://arxiv.org/abs/2304.02841) | :heavy_minus_sign: |
| Divide and Conquer: 3D Point Cloud Instance Segmentation with Point-Wise Binarization | [![GitHub](https://img.shields.io/github/stars/weiguangzhao/PBNet)](https://github.com/weiguangzhao/PBNet) | [![arXiv](https://img.shields.io/badge/arXiv-2207.11209-b31b1b.svg)](https://arxiv.org/abs/2207.11209) | :heavy_minus_sign: |
| Point2Mask: Point-Supervised Panoptic Segmentation via Optimal Transport | [![GitHub](https://img.shields.io/github/stars/LiWentomng/Point2Mask)](https://github.com/LiWentomng/Point2Mask) | [![arXiv](https://img.shields.io/badge/arXiv-2308.01779-b31b1b.svg)](https://arxiv.org/abs/2308.01779) | :heavy_minus_sign: |
| Handwritten and Printed Text Segmentation: A Signature Case Study | [![SignaTR6K](https://img.shields.io/badge/SignaTR6K-dataset-20BEFF.svg)](https://forms.office.com/r/2a5RDg7cAY) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07887-b31b1b.svg)](https://arxiv.org/abs/2307.07887) | :heavy_minus_sign: |
| Semantic-Aware Template Learning via Part Deformation Consistency | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.11916-b31b1b.svg)](https://arxiv.org/abs/2308.11916) | :heavy_minus_sign: |
| LeaF: Learning Frames for 4D Point Cloud Sequence Understanding | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MARS: Model-Agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/shjo-april/MARS)](https://github.com/shjo-april/MARS) | [![arXiv](https://img.shields.io/badge/arXiv-2304.09913-b31b1b.svg)](https://arxiv.org/abs/2304.09913) | :heavy_minus_sign: |
| USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.07806-b31b1b.svg)](https://arxiv.org/abs/2303.07806) | :heavy_minus_sign: |
| Production-Level Video Segmentation from Few Annotated Frames | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://max810.github.io/xmem2-project-page/) <br /> [![GitHub](https://img.shields.io/github/stars/max810/XMem2)](https://github.com/max810/XMem2) | [![arXiv](https://img.shields.io/badge/arXiv-2307.15958-b31b1b.svg)](https://arxiv.org/abs/2307.15958) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3X3TUP4vKcc) |
| Î£IGMA: Scale-Invariant Global Sparse Shape Matching | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.08393-b31b1b.svg)](https://arxiv.org/abs/2308.08393) | :heavy_minus_sign: |
| Self-Calibrated Cross Attention Network for Few-Shot Segmentation | [![GitHub](https://img.shields.io/github/stars/Sam1224/SCCAN)](https://github.com/Sam1224/SCCAN) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09294-b31b1b.svg)](https://arxiv.org/abs/2308.09294) | :heavy_minus_sign: |
| Multi-Granularity Interaction Simulation for Unsupervised Interactive Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.13399-b31b1b.svg)](https://arxiv.org/abs/2303.13399) | :heavy_minus_sign: |
| Texture Learning Domain Randomization for Domain Generalized Segmentation | [![GitHub](https://img.shields.io/github/stars/ssssshwan/TLDR)](https://github.com/ssssshwan/TLDR) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11546-b31b1b.svg)](https://arxiv.org/abs/2303.11546) | :heavy_minus_sign: |
| Unsupervised Video Object Segmentation with Online Adversarial Self-Tuning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Exploring Open-Vocabulary Semantic Segmentation without Human Labels | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00450-b31b1b.svg)](https://arxiv.org/abs/2306.00450) | :heavy_minus_sign: |
| RbA: Segmenting Unknown Regions Rejected by All | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kuis-ai.github.io/RbA/) <br /> [![GitHub](https://img.shields.io/github/stars/NazirNayal8/RbA)](https://github.com/NazirNayal8/RbA) | [![arXiv](https://img.shields.io/badge/arXiv-2211.14293-b31b1b.svg)](https://arxiv.org/abs/2211.14293) | :heavy_minus_sign: |
| SEMPART: Self-Supervised Multi-Resolution Partitioning of Image Semantics | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Multi-Object Discovery by Low-Dimensional Object Motion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kuis-ai.github.io/multi-object-segmentation/) <br /> [![GitHub](https://img.shields.io/github/stars/sadrasafa/multi-object-segmentation)](https://github.com/sadrasafa/multi-object-segmentation) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08027-b31b1b.svg)](https://arxiv.org/abs/2307.08027) | :heavy_minus_sign: |
| MemorySeg: Online LiDAR Semantic Segmentation with a Latent Memory | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Treating Pseudo-Labels Generation as Image Matting for Weakly Supervised Semantic Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| BoxSnake: Polygonal Instance Segmentation with Box Supervision | [![GitHub](https://img.shields.io/github/stars/Yangr116/BoxSnake)](https://github.com/Yangr116/BoxSnake) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11630-b31b1b.svg)](https://arxiv.org/abs/2303.11630) | :heavy_minus_sign: |
| Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.01045-b31b1b.svg)](https://arxiv.org/abs/2308.01045) | :heavy_minus_sign: |
| Instance Neural Radiance Field | [![GitHub](https://img.shields.io/github/stars/lyclyc52/Instance_NeRF)](https://github.com/lyclyc52/Instance_NeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2304.04395-b31b1b.svg)](https://arxiv.org/abs/2304.04395) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wW9Bme73coI) |
| Global Knowledge Calibration for Fast Open-Vocabulary Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.09181-b31b1b.svg)](https://arxiv.org/abs/2303.09181) | :heavy_minus_sign: |
| Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.12350-b31b1b.svg)](https://arxiv.org/abs/2308.12350) | :heavy_minus_sign: |
| Boosting Semantic Segmentation from an Explicit Class Embedding's Perspective | [![gitee](https://gitee-badge.vercel.app/svg/stars/mindspore/models)](https://gitee.com/mindspore/models) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12894-b31b1b.svg)](https://arxiv.org/abs/2308.12894) | :heavy_minus_sign: |
| The Making and Breaking of Camouflage | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| CoinSeg: Contrast Inter- and Intra- Class Representations for Incremental Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://meowuu7.github.io/few-arti-obj-gen/) <br /> [![GitHub](https://img.shields.io/github/stars/Meowuu7/few-arti-gen)](https://github.com/Meowuu7/few-arti-gen) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10898-b31b1b.svg)](https://arxiv.org/abs/2308.10898) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=p8x3GN3VSPE) |
| HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.10460-b31b1b.svg)](https://arxiv.org/abs/2301.10460) | :heavy_minus_sign: |
| FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation | [![GitHub](https://img.shields.io/github/stars/TY-Shi/FreeCOS)](https://github.com/TY-Shi/FreeCOS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07245-b31b1b.svg)](https://arxiv.org/abs/2307.07245) | :heavy_minus_sign: |
| MasQCLIP for Open-Vocabulary Universal Image Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| CTVIS: Consistent Training for Online Video Instance Segmentation | [![GitHub](https://img.shields.io/github/stars/KainingYing/CTVIS)](https://github.com/KainingYing/CTVIS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12616-b31b1b.svg)](https://arxiv.org/abs/2307.12616) | :heavy_minus_sign: |
| A Simple Framework for Panoptic Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Spectrum-Guided Multi-Granularity Referring Video Object Segmentation | [![GitHub](https://img.shields.io/github/stars/bo-miao/SgMg)](https://github.com/bo-miao/SgMg) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13537-b31b1b.svg)](https://arxiv.org/abs/2307.13537) | :heavy_minus_sign: |
| Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/WangChangqi98/CSS)](https://github.com/WangChangqi98/CSS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09755-b31b1b.svg)](https://arxiv.org/abs/2307.09755) | :heavy_minus_sign: |
| Adaptive Superpixel for Active Learning in Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.16817-b31b1b.svg)](https://arxiv.org/abs/2303.16817) | :heavy_minus_sign: |
| Multimodal Variational Auto-Encoder based Audio-Visual Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Isomer: Isomerous Transformer for Zero-Shot Video Object Segmentation | [![GitHub](https://img.shields.io/github/stars/DLUT-yyc/Isomer)](https://github.com/DLUT-yyc/Isomer) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06693-b31b1b.svg)](https://arxiv.org/abs/2308.06693) | :heavy_minus_sign: |
| 2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jimmy15923.github.io/mit_web/) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://vllab.cs.nctu.edu.tw/images/paper/iccv-yang23.pdf) | :heavy_minus_sign: |
| Foreground-Background Separation through Concept Distillation from Generative Image Foundation Models | [![GitHub](https://img.shields.io/github/stars/MischaD/fobadiffusion)](https://github.com/MischaD/fobadiffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2212.14306-b31b1b.svg)](https://arxiv.org/abs/2212.14306) | :heavy_minus_sign: |
| SegPrompt: Boosting Open-World Segmentation via Category-Level Prompt Learning | [![GitHub](https://img.shields.io/github/stars/aim-uofa/SegPrompt)](https://github.com/aim-uofa/SegPrompt) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06531-b31b1b.svg)](https://arxiv.org/abs/2308.06531) | :heavy_minus_sign: |
| Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yeren123455.github.io/SIRST-Single-Point-Supervision/) <br /> [![GitHub](https://img.shields.io/github/stars/YeRen123455/SIRST-Single-Point-Supervision)](https://github.com/YeRen123455/SIRST-Single-Point-Supervision) | [![arXiv](https://img.shields.io/badge/arXiv-2304.04442-b31b1b.svg)](https://arxiv.org/abs/2304.04442) | :heavy_minus_sign: |
| A Simple Framework for Open-Vocabulary Segmentation and Detection | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/OpenSeeD)](https://github.com/IDEA-Research/OpenSeeD) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08131-b31b1b.svg)](https://arxiv.org/abs/2303.08131) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z4gsQw2n7iM) |
| Source-Free Depth for Object Pop-Out | [![GitHub](https://img.shields.io/github/stars/Zongwei97/PopNet)](https://github.com/Zongwei97/PopNet) | [![arXiv](https://img.shields.io/badge/arXiv-2212.05370-b31b1b.svg)](https://arxiv.org/abs/2212.05370) | :heavy_minus_sign: |
| DynaMITe: Dynamic Query Bootstrapping for Multi-Object Interactive Segmentation Transformer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://amitrana001.github.io/DynaMITe/) <br /> [![GitHub](https://img.shields.io/github/stars/amitrana001/DynaMITe)](https://github.com/amitrana001/DynaMITe) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06668-b31b1b.svg)](https://arxiv.org/abs/2304.06668) | :heavy_minus_sign: |
| Atmospheric Transmission and Thermal Inertia Induced Blind Road Segmentation with a Large-Scale Dataset TBRSD | [![GitHub](https://img.shields.io/github/stars/chenjzBUAA/TBRSD)](https://github.com/chenjzBUAA/TBRSD) | :heavy_minus_sign: | :heavy_minus_sign: |
| Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Homography Guided Temporal Fusion for Road Line and Marking Segmentation | [![GitHub](https://img.shields.io/github/stars/ShanWang-Shan/HomoFusion)](https://github.com/ShanWang-Shan/HomoFusion) | :heavy_minus_sign: | :heavy_minus_sign: |
| Zero-Shot Semantic Segmentation with Decoupled One-Shot Network | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TCOVIS: Temporally Consistent Online Video Instance Segmentation | [![GitHub](https://img.shields.io/github/stars/jun-long-li/TCOVIS)](https://github.com/jun-long-li/TCOVIS) | :heavy_minus_sign: | :heavy_minus_sign: |
| FPR: False Positive Rectification for Weakly Supervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/mt-cly/FPR)](https://github.com/mt-cly/FPR) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://www4.comp.polyu.edu.hk/~cslzhang/paper/ICCV23-FPR.pdf) | :heavy_minus_sign: |
| Stochastic Segmentation with Conditional Categorical Diffusion Models | [![GitHub](https://img.shields.io/github/stars/LarsDoorenbos/ccdm-stochastic-segmentation)](https://github.com/LarsDoorenbos/ccdm-stochastic-segmentation) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08888-b31b1b.svg)](https://arxiv.org/abs/2303.08888) | :heavy_minus_sign: |
| SegGPT: Segmenting Everything in Context | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/baaivision/Painter/tree/main/SegGPT) <br /> [![GitHub](https://img.shields.io/github/stars/baaivision/Painter)](https://github.com/baaivision/Painter) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-SegGPT-FFD21F.svg)](https://huggingface.co/spaces/BAAI/SegGPT) | [![arXiv](https://img.shields.io/badge/arXiv-2304.03284-b31b1b.svg)](https://arxiv.org/abs/2304.03284) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zxwH0dUBKis) |
| Open-Vocabulary Panoptic Segmentation with Embedding Modulation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.11324-b31b1b.svg)](https://arxiv.org/abs/2303.11324) | :heavy_minus_sign: |
| Residual Pattern Learning for Pixel-Wise Out-of-Distribution Detection in Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/yyliu01/RPL)](https://github.com/yyliu01/RPL) | [![arXiv](https://img.shields.io/badge/arXiv-2211.14512-b31b1b.svg)](https://arxiv.org/abs/2211.14512) | :heavy_minus_sign: |
| Zero-Guidance Segmentation using Zero Segment Labels | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zero-guide-seg.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13396-b31b1b.svg)](https://arxiv.org/abs/2303.13396) | :heavy_minus_sign: |
| Model Calibration in Dense Classification with Adaptive Label Perturbation | [![GitHub](https://img.shields.io/github/stars/Carlisle-Liu/ASLP)](https://github.com/Carlisle-Liu/ASLP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13539-b31b1b.svg)](https://arxiv.org/abs/2307.13539) | :heavy_minus_sign: |
| Enhanced Soft Label for Semi-Supervised Semantic Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.04829-b31b1b.svg)](https://arxiv.org/abs/2308.04829) | :heavy_minus_sign: |
| DiffuMask: Synthesizing Images with Pixel-Level Annotations for Semantic Segmentation using Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://weijiawu.github.io/DiffusionMask/) <br /> [![GitHub](https://img.shields.io/github/stars/weijiawu/DiffuMask)](https://github.com/weijiawu/DiffuMask) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11681-b31b1b.svg)](https://arxiv.org/abs/2303.11681) | :heavy_minus_sign: |
| Alignment Before Aggregation: Trajectory Memory Retrieval Network for Video Object Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Semi-Supervised Semantic Segmentation under Label Noise via Diverse Learning Groups | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets | [![GitHub](https://img.shields.io/github/stars/csimo005/SUMMIT)](https://github.com/csimo005/SUMMIT) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11880-b31b1b.svg)](https://arxiv.org/abs/2308.11880) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LDlLq9IdoAw) |
| Class-Incremental Continual Learning for Instance Segmentation with Image-Level Weak Supervision | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Coarse-to-Fine Amodal Segmentation with Shape Prior | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jianxgao.github.io/C2F-Seg/) <br /> [![GitHub](https://img.shields.io/github/stars/JianxGao/C2F-Seg)](https://github.com/JianxGao/C2F-Seg) | [![arXiv](https://img.shields.io/badge/arXiv-2308.16825-b31b1b.svg)](https://arxiv.org/abs/2308.16825) | :heavy_minus_sign: |
| Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-Centric Representation | [![GitHub](https://img.shields.io/github/stars/kfan21/EoRaS)](https://github.com/kfan21/EoRaS) | :heavy_minus_sign: | :heavy_minus_sign: |
| DVIS: Decoupled Video Instance Segmentation Framework | [![GitHub](https://img.shields.io/github/stars/zhang-tao-whu/DVIS)](https://github.com/zhang-tao-whu/DVIS) | [![arXiv](https://img.shields.io/badge/arXiv-2306.03413-b31b1b.svg)](https://arxiv.org/abs/2306.03413) | :heavy_minus_sign: |
| 3D Segmentation of Humans in Point Clouds with Synthetic Data | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://human-3d.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2212.00786-b31b1b.svg)](https://arxiv.org/abs/2212.00786) | :heavy_minus_sign: |
| WaterMask: Instance Segmentation for Underwater Imagery | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Decoupled or End-to-End Trained Video Segmentation if Target Data is Scarce? | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Categorization

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Cross Contrasting Feature Perturbation for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/hackmebroo/CCFP)](https://github.com/hackmebroo/CCFP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12502-b31b1b.svg)](https://arxiv.org/abs/2307.12502) | :heavy_minus_sign: |
| Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance | :heavy_minus_sign: | :heavy_minus_sign:  | :heavy_minus_sign: |
| CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.16634-b31b1b.svg)](https://arxiv.org/abs/2307.16634) | :heavy_minus_sign: |
| RankMixup: Ranking-based Mixup Training for Network Calibration | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cvlab.yonsei.ac.kr/projects/RankMixup/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11990-b31b1b.svg)](https://arxiv.org/abs/2308.11990) | :heavy_minus_sign: |
| Label-Noise Learning with Intrinsically Long-Tailed Data | [![GitHub](https://img.shields.io/github/stars/Wakings/TABASCO)](https://github.com/Wakings/TABASCO) | [![arXiv](https://img.shields.io/badge/arXiv-2208.09833-b31b1b.svg)](https://arxiv.org/abs/2208.09833) | :heavy_minus_sign: |
| Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition | [![GitHub](https://img.shields.io/github/stars/starrycos/PAINet)](https://github.com/starrycos/PAINet) | :heavy_minus_sign: | :heavy_minus_sign: |
| Rethinking Mobile Block for Efficient Attention-based Models | [![GitHub](https://img.shields.io/github/stars/zhangzjn/EMO)](https://github.com/zhangzjn/EMO) | [![arXiv](https://img.shields.io/badge/arXiv-2301.01146-b31b1b.svg)](https://arxiv.org/abs/2301.01146) | :heavy_minus_sign: |
| Read-Only Prompt Optimization for Vision-Language Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/mlvlab/RPO)](https://github.com/mlvlab/RPO) | [![arXiv](https://img.shields.io/badge/arXiv-2308.14960-b31b1b.svg)](https://arxiv.org/abs/2308.14960) | :heavy_minus_sign: |
| Understanding Self-Attention Mechanism via Dynamical System Perspective | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.09939-b31b1b.svg)](https://arxiv.org/abs/2308.09939) | :heavy_minus_sign: |
| Learning in Imperfect Environment: Multi-Label Classification with Long-Tailed Distribution and Partial Labels | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.10539-b31b1b.svg)](https://arxiv.org/abs/2304.10539) | :heavy_minus_sign: |
| What do Neural Networks Learn in Image Classification? A Frequency Shortcut Perspective | [![GitHub](https://img.shields.io/github/stars/nis-research/nn-frequency-shortcuts)](https://github.com/nis-research/nn-frequency-shortcuts) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09829-b31b1b.svg)](https://arxiv.org/abs/2307.09829) | :heavy_minus_sign: |
| Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity | [![GitHub](https://img.shields.io/github/stars/ltong1130ztr/HAFrame)](https://github.com/ltong1130ztr/HAFrame) | [![arXiv](https://img.shields.io/badge/arXiv-2303.05689-b31b1b.svg)](https://arxiv.org/abs/2303.05689) | :heavy_minus_sign: |
| Unified Out-of-Distribution Detection: A Model-Specific Perspective | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.06813-b31b1b.svg)](https://arxiv.org/abs/2304.06813) | :heavy_minus_sign: |
| A Unified Framework for Robustness on Diverse Sampling Errors | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Scene-Aware Label Graph Learning for Multi-Label Image Classification | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Holistic Label Correction for Noisy Multi-Label Classification | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Strip-MLP: Efficient Token Interaction for Vision MLP | [![GitHub](https://img.shields.io/github/stars/Med-Process/Strip_MLP)](https://github.com/Med-Process/Strip_MLP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11458-b31b1b.svg)](https://arxiv.org/abs/2307.11458) | :heavy_minus_sign: |
| EQ-Net: Elastic Quantization Neural Networks | [![GitHub](https://img.shields.io/github/stars/xuke225/EQ-Net)](https://github.com/xuke225/EQ-Net) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07650-b31b1b.svg)](https://arxiv.org/abs/2308.07650) | :heavy_minus_sign: |
| Data-Free Knowledge Distillation for Fine-Grained Vision Categorization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Shift from Texture-Bias to Shape-Bias: edge Deformation-based Augmentation for Robust Object Recognition | [![GitHub](https://img.shields.io/github/stars/C0notSilly/-ICCV-23-Edge-Deformation-based-Online-Augmentation)](https://github.com/C0notSilly/-ICCV-23-Edge-Deformation-based-Online-Augmentation) | :heavy_minus_sign: | :heavy_minus_sign: |
| Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition | [![GitHub](https://img.shields.io/github/stars/leeisack/Latent-OFER)](https://github.com/leeisack/Latent-OFER) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11404-b31b1b.svg)](https://arxiv.org/abs/2307.11404) | :heavy_minus_sign: |
| DR-Tune: Improving Fine-Tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration | [![GitHub](https://img.shields.io/github/stars/weeknan/DR-Tune)](https://github.com/weeknan/DR-Tune) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12058-b31b1b.svg)](https://arxiv.org/abs/2308.12058) | :heavy_minus_sign: |
| Understanding the Feature Norm for Out-of-Distribution Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Multi-View Active Fine-Grained Visual Recognition | [![GitHub](https://img.shields.io/github/stars/PRIS-CV/AFGR)](https://github.com/PRIS-CV/AFGR) | [![arXiv](https://img.shields.io/badge/arXiv-2206.01153-b31b1b.svg)](https://arxiv.org/abs/2206.01153) | :heavy_minus_sign: |
| DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-Trained Diffusion Models | [![GitHub](https://img.shields.io/github/stars/cure-lab/DiffGuard)](https://github.com/cure-lab/DiffGuard) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07687-b31b1b.svg)](https://arxiv.org/abs/2308.07687) | :heavy_minus_sign: |
| Task-Aware Adaptive Learning for Cross-Domain Few-Shot Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Improving Adversarial Robustness of Masked Autoencoders via Test-Time Frequency-Domain Prompting | [![GitHub](https://img.shields.io/github/stars/shikiw/RobustMAE)](https://github.com/shikiw/RobustMAE) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10315-b31b1b.svg)](https://arxiv.org/abs/2308.10315) | :heavy_minus_sign: |
| Saliency Regularization for Self-Training with Partial Annotations | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Gabor Texture Features for Fine-Grained Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.05396-b31b1b.svg)](https://arxiv.org/abs/2308.05396) | :heavy_minus_sign: |
| UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/UniFormerV2)](https://github.com/OpenGVLab/UniFormerV2) | [![arXiv](https://img.shields.io/badge/arXiv-2211.09552-b31b1b.svg)](https://arxiv.org/abs/2211.09552) | :heavy_minus_sign: |
| RankMatch: Fostering Confidence and Consistency in Learning with Noisy Labels | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MetaGCD: Learning to Continually Learn in Generalized Category Discovery | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.11063-b31b1b.svg)](https://arxiv.org/abs/2308.11063) | :heavy_minus_sign: |
| FerKD: Surgical Label Adaptation for Efficient Distillation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Point-Query Quadtree for Crowd Counting, Localization, and more | [![GitHub](https://img.shields.io/github/stars/cxliu0/PET)](https://github.com/cxliu0/PET) | [![arXiv](https://img.shields.io/badge/arXiv-2308.13814-b31b1b.svg)](https://arxiv.org/abs/2308.13814) | :heavy_minus_sign: |
| Nearest Neighbor Guidance for Out-of-Distribution Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Bayesian Optimization Meets Self-Distillation | [![GitHub](https://img.shields.io/github/stars/sooperset/boss)](https://github.com/sooperset/boss) | [![arXiv](https://img.shields.io/badge/arXiv-2304.12666-b31b1b.svg)](https://arxiv.org/abs/2304.12666) | :heavy_minus_sign: |
| When Prompt-based Incremental Learning does not Meet Strong Pretraining | [![GitHub](https://img.shields.io/github/stars/TOM-tym/APG)](https://github.com/TOM-tym/APG) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10445-b31b1b.svg)](https://arxiv.org/abs/2308.10445) | :heavy_minus_sign: |
| When to Learn what: Model-Adaptive Data Augmentation Curriculum | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Parametric Information Maximization for Generalized Category Discovery | [![GitHub](https://img.shields.io/github/stars/ThalesGroup/pim-generalized-category-discovery)](https://github.com/ThalesGroup/pim-generalized-category-discovery) | [![arXiv](https://img.shields.io/badge/arXiv-2212.00334-b31b1b.svg)](https://arxiv.org/abs/2212.00334) | :heavy_minus_sign: |
| Boosting Few-Shot Action Recognition with Graph-Guided Hybrid Matching | [![GitHub](https://img.shields.io/github/stars/jiazheng-xing/GgHM)](https://github.com/jiazheng-xing/GgHM) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09346-b31b1b.svg)](https://arxiv.org/abs/2308.09346) | :heavy_minus_sign: |
| Domain Generalization via Rationale Invariance | [![GitHub](https://img.shields.io/github/stars/liangchen527/RIDG)](https://github.com/liangchen527/RIDG) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11158-b31b1b.svg)](https://arxiv.org/abs/2308.11158) | :heavy_minus_sign: |
| Masked Spiking Transformer | [![GitHub](https://img.shields.io/github/stars/bic-L/Masked-Spiking-Transformer)](https://github.com/bic-L/Masked-Spiking-Transformer) | [![arXiv](https://img.shields.io/badge/arXiv-2210.01208-b31b1b.svg)](https://arxiv.org/abs/2210.01208) | :heavy_minus_sign: |
| Prototype Reminiscence and Augmented Asymmetric Knowledge Aggregation for Non-Exemplar Class-Incremental Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Distilled Reverse Attention Network for Open-World Compositional Zero-Shot Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.00404-b31b1b.svg)](https://arxiv.org/abs/2303.00404) | :heavy_minus_sign: |
| Candidate-Aware Selective Disambiguation based on Normalized Entropy for Instance-Dependent Partial-Label Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No | [![GitHub](https://img.shields.io/github/stars/xmed-lab/CLIPN)](https://github.com/xmed-lab/CLIPN) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12213-b31b1b.svg)](https://arxiv.org/abs/2308.12213) | :heavy_minus_sign: |
| Self-Similarity Driven Scale-Invariant Learning for Weakly Supervised Person Search | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.12986-b31b1b.svg)](https://arxiv.org/abs/2302.12986) | :heavy_minus_sign: |
| Sample-Wise Label Confidence Incorporation for Learning with Noisy Labels | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Combating Noisy Labels with Sample Selection by Mining High-Discrepancy Examples | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Spatial-Aware Token for Weakly Supervised Object Localization | [![GitHub](https://img.shields.io/github/stars/wpy1999/SAT)](https://github.com/wpy1999/SAT) | [![arXiv](https://img.shields.io/badge/arXiv-2303.10438-b31b1b.svg)](https://arxiv.org/abs/2303.10438) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Explainable AI for CV

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Towards Improved Input Masking for Convolutional Neural Networks | [![GitHub](https://img.shields.io/github/stars/SriramB-98/layer_masking)](https://github.com/SriramB-98/layer_masking) | [![arXiv](https://img.shields.io/badge/arXiv-2211.14646-b31b1b.svg)](https://arxiv.org/abs/2211.14646) | :heavy_minus_sign: |
| PDiscoNet: Semantically Consistent Part Discovery for Fine-Grained Recognition | [![GitHub](https://img.shields.io/github/stars/robertdvdk/part_detection)](https://github.com/robertdvdk/part_detection) | [![HAL Science](https://img.shields.io/badge/hal-science-040060.svg)](https://hal.inrae.fr/hal-04183747) | :heavy_minus_sign: |
| Corrupting Neuron Explanations of Deep Visual Features | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ICICLE: Interpretable Class Incremental Continual Learning | [![GitHub](https://img.shields.io/github/stars/gmum/ICICLE)](https://github.com/gmum/ICICLE) | [![arXiv](https://img.shields.io/badge/arXiv-2303.07811-b31b1b.svg)](https://arxiv.org/abs/2303.07811) | :heavy_minus_sign: |
| ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.eml-unitue.de/publication/ProbVLM) <br /> [![GitHub](https://img.shields.io/github/stars/ExplainableML/ProbVLM)](https://github.com/ExplainableML/ProbVLM) | [![arXiv](https://img.shields.io/badge/arXiv-2307.00398-b31b1b.svg)](https://arxiv.org/abs/2307.00398) | :heavy_minus_sign: |
| Out-of-Distribution Detection for Monocular Depth Estimation | [![GitHub](https://img.shields.io/github/stars/jhornauer/mde_ood)](https://github.com/jhornauer/mde_ood) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06072-b31b1b.svg)](https://arxiv.org/abs/2308.06072) | :heavy_minus_sign: |
| Using Explanations to Guide Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.11932-b31b1b.svg)](https://arxiv.org/abs/2303.11932) | :heavy_minus_sign: |
| Rosetta Neurons: Mining the Common Units in a Model Zoo | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yossigandelsman.github.io/rosetta_neurons/) <br /> [![GitHub](https://img.shields.io/github/stars/yossigandelsman/rosetta_neurons)](https://github.com/yossigandelsman/rosetta_neurons) | [![arXiv](https://img.shields.io/badge/arXiv-2306.09346-b31b1b.svg)](https://arxiv.org/abs/2306.09346) | :heavy_minus_sign: |
| Prototype-based Dataset Comparison | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nanne.github.io/ProtoSim/) <br /> [![GitHub](https://img.shields.io/github/stars/Nanne/ProtoSim)](https://github.com/Nanne/ProtoSim) | [![arXiv](https://img.shields.io/badge/arXiv-2309.02401-b31b1b.svg)](https://arxiv.org/abs/2309.02401) | :heavy_minus_sign: |
| Learning to Identify Critical States for Reinforcement Learning from Videos | [![GitHub](https://img.shields.io/github/stars/AI-Initiative-KAUST/VideoRLCS)](https://github.com/AI-Initiative-KAUST/VideoRLCS) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07795-b31b1b.svg)](https://arxiv.org/abs/2308.07795) | :heavy_minus_sign: |
| Leaping Into Memories: Space-Time Deep Feature Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://alexandrosstergiou.github.io/project_pages/LEAPS/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/alexandrosstergiou/Leaping-Into-Memories)](https://github.com/alexandrosstergiou/Leaping-Into-Memories) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09941-b31b1b.svg)](https://arxiv.org/abs/2303.09941) | :heavy_minus_sign: |
| MAGI: Multi-Annotated Explanation-Guided Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability | [![GitHub](https://img.shields.io/github/stars/havelhuang/Eval_XAI_Robustness)](https://github.com/havelhuang/Eval_XAI_Robustness) | [![arXiv](https://img.shields.io/badge/arXiv-2208.09418-b31b1b.svg)](https://arxiv.org/abs/2208.09418) | :heavy_minus_sign: |
| Do BLIP and Stable Diffusion Understand Each Other? | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dalleflamingo.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2212.12249-b31b1b.svg)](https://arxiv.org/abs/2212.12249) | :heavy_minus_sign: |
| Evaluation and Improvement of Interpretability for Self-Explainable Part-Prototype Networks | [![GitHub](https://img.shields.io/github/stars/hqhQAQ/EvalProtoPNet)](https://github.com/hqhQAQ/EvalProtoPNet) | [![arXiv](https://img.shields.io/badge/arXiv-2212.05946-b31b1b.svg)](https://arxiv.org/abs/2212.05946) | :heavy_minus_sign: |
| MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope | [![GitHub](https://img.shields.io/github/stars/buyeah1109/MoreauGrad)](https://github.com/buyeah1109/MoreauGrad) | [![arXiv](https://img.shields.io/badge/arXiv-2302.05294-b31b1b.svg)](https://arxiv.org/abs/2302.05294) | :heavy_minus_sign: |
| Towards Understanding the Generalization of Deepfake Detectors from a Game-Theoretical View | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Counterfactual-based Saliency Map: Towards Visual Contrastive Explanations for Neural Networks | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Beyond Single Path Integrated Gradients for Reliable Input Attribution via Randomized Path Sampling | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Support and Trivial Prototypes for Interpretable Image Classification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.04011-b31b1b.svg)](https://arxiv.org/abs/2301.04011) | :heavy_minus_sign: |
| Visual Explanations via Iterated Integrated Gradients | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Neural Generative Models

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://energy-based-model.github.io/unsupervised-concept-discovery/) <br /> [![GitHub](https://img.shields.io/github/stars/nanlliu/Unsupervised-Compositional-Concepts-Discovery)](https://github.com/nanlliu/Unsupervised-Compositional-Concepts-Discovery) | [![arXiv](https://img.shields.io/badge/arXiv-2306.05357-b31b1b.svg)](https://arxiv.org/abs/2306.05357) | :heavy_minus_sign: |
| Better Aligning Text-to-Image Models with Human Preference | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tgxs002.github.io/align_sd_web/) <br /> [![GitHub](https://img.shields.io/github/stars/tgxs002/align_sd)](https://github.com/tgxs002/align_sd) | [![arXiv](https://img.shields.io/badge/arXiv-2303.14420-b31b1b.svg)](https://arxiv.org/abs/2303.14420) | :heavy_minus_sign: |
| DLT: Conditioned Layout Generation with Joint Discrete-Continuous Diffusion Layout Transformer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wix-incubator.github.io/DLT/) <br /> [![GitHub](https://img.shields.io/github/stars/wix-incubator/DLT)](https://github.com/wix-incubator/DLT) | [![arXiv](https://img.shields.io/badge/arXiv-2303.03755-b31b1b.svg)](https://arxiv.org/abs/2303.03755) | :heavy_minus_sign: |
| Anti-DreamBooth: Protecting users from Personalized Text-to-Image Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://anti-dreambooth.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/VinAIResearch/Anti-DreamBooth)](https://github.com/VinAIResearch/Anti-DreamBooth) | [![arXiv](https://img.shields.io/badge/arXiv-2303.15433-b31b1b.svg)](https://arxiv.org/abs/2303.15433) | :heavy_minus_sign: |
| GECCO: Geometrically-Conditioned Point Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jatentaki.github.io/publication/10-03-2023) | [![arXiv](https://img.shields.io/badge/arXiv-2303.05916-b31b1b.svg)](https://arxiv.org/abs/2303.05916) | :heavy_minus_sign: |
| DiffDreamer: Towards Consistent Unsupervised Single-View Scene Extrapolation with Conditional Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://primecai.github.io/diffdreamer) <br /> [![GitHub](https://img.shields.io/github/stars/primecai/DiffDreamer)](https://github.com/primecai/DiffDreamer) | [![arXiv](https://img.shields.io/badge/arXiv-2211.12131-b31b1b.svg)](https://arxiv.org/abs/2211.12131) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UukyiAqlwcw) |
| Controllable Human Motion Synthesis via Guided Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://korrawe.github.io/gmd-project/) <br /> [![GitHub](https://img.shields.io/github/stars/korrawe/guided-motion-diffusion)](https://github.com/korrawe/guided-motion-diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12577-b31b1b.svg)](https://arxiv.org/abs/2305.12577) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=giw0pLIKdsA) |
| COOP: Decoupling and Coupling of Whole-Body Grasping Pose Generation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.13754-b31b1b.svg)](https://arxiv.org/abs/2306.13754) | :heavy_minus_sign: |
| StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-Shot and Few-Shot Domain Adaptation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2212.10229-b31b1b.svg)](https://arxiv.org/abs/2212.10229) | :heavy_minus_sign: |
| GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jeffreyxiang.github.io/GRAM-HD/) | [![arXiv](https://img.shields.io/badge/arXiv-2206.07255-b31b1b.svg)](https://arxiv.org/abs/2206.07255) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Uqzs4uN6v8M) |
| Your Diffusion Model is Secretly a Zero-Shot Classifier | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffusion-classifier.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/diffusion-classifier/diffusion-classifier)](https://github.com/diffusion-classifier/diffusion-classifier) | [![arXiv](https://img.shields.io/badge/arXiv-2303.16203-b31b1b.svg)](https://arxiv.org/abs/2303.16203) | :heavy_minus_sign: |
| Learning Hierarchical Features with Joint Latent Space Energy-based Prior | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2203.07706-b31b1b.svg)](https://arxiv.org/abs/2203.07706) | :heavy_minus_sign: |
| Landscape Learning for Neural Network Inversion | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2206.09027-b31b1b.svg)](https://arxiv.org/abs/2206.09027) | :heavy_minus_sign: |
| Diffusion in Style | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://light.princeton.edu/publication/diffusion-sdf/) <br /> [![GitHub](https://img.shields.io/github/stars/princeton-computational-imaging/Diffusion-SDF)](https://github.com/princeton-computational-imaging/Diffusion-SDF) | [![arXiv](https://img.shields.io/badge/arXiv-2211.13757-b31b1b.svg)](https://arxiv.org/abs/2211.13757) | :heavy_minus_sign: |
| GETAvatar: Generative Textured Meshes for Animatable Human Avatars | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| A-STAR: Test-Time <i>A</i>ttention <i>S</i>egrega<i>t</i>ion and <i>R</i>etention for Text-to-Image Synthesis | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.14544-b31b1b.svg)](https://arxiv.org/abs/2306.14544) | :heavy_minus_sign: |
| TF-ICON: Diffusion-based Training-Free Cross-Domain Image Composition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shilin-lu.github.io/tf-icon.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Shilin-LU/TF-ICON)](https://github.com/Shilin-LU/TF-ICON) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12493-b31b1b.svg)](https://arxiv.org/abs/2307.12493) | :heavy_minus_sign: |
| Breaking The Limits of Text-Conditioned 3D Motion Synthesis with Elaborative Descriptions | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://barquerogerman.github.io/BeLFusion/) <br /> [![GitHub](https://img.shields.io/github/stars/BarqueroGerman/BeLFusion)](https://github.com/BarqueroGerman/BeLFusion) | [![arXiv](https://img.shields.io/badge/arXiv-2211.14304-b31b1b.svg)](https://arxiv.org/abs/2211.14304) | :heavy_minus_sign: |
| Delta Denoising Score | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://delta-denoising-score.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2304.07090-b31b1b.svg)](https://arxiv.org/abs/2304.07090) | :heavy_minus_sign: |
| Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://seanchenxy.github.io/Mimic3DWeb/) <br /> [![GitHub](https://img.shields.io/github/stars/SeanChenxy/Mimic3D)](https://github.com/SeanChenxy/Mimic3D) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09036-b31b1b.svg)](https://arxiv.org/abs/2303.09036) | :heavy_minus_sign: |
| DreamBooth3D: Subject-Driven Text-to-3D Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dreambooth3d.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13508-b31b1b.svg)](https://arxiv.org/abs/2303.13508) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kKVDrbfvOoA) |
| Feature Proliferation the Cancer in StyleGAN and its Treatments | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Unsupervised Facial Performance Editing via Vector-Quantized StyleGAN Representations | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| 3D-Aware Image Generation using 2D Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jeffreyxiang.github.io/ivid/) <br /> [![GitHub](https://img.shields.io/github/stars/JeffreyXiang/ivid)](https://github.com/JeffreyXiang/ivid) | [![arXiv](https://img.shields.io/badge/arXiv-2303.17905-b31b1b.svg)](https://arxiv.org/abs/2303.17905) | :heavy_minus_sign: |
| Neural Collage Transfer: Artistic Reconstruction via Material Manipulation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption | [![GitHub](https://img.shields.io/github/stars/sjtuplayer/few-shot-diffusion)](https://github.com/sjtuplayer/few-shot-diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2309.03729-b31b1b.svg)](https://arxiv.org/abs/2309.03729) | :heavy_minus_sign: |
| Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lakonik.github.io/ssdnerf/) <br /> [![GitHub](https://img.shields.io/github/stars/Lakonik/SSDNeRF)](https://github.com/Lakonik/SSDNeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06714-b31b1b.svg)](https://arxiv.org/abs/2304.06714) | :heavy_minus_sign: |
| Erasing Concepts from Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://erasing.baulab.info/) <br /> [![GitHub](https://img.shields.io/github/stars/rohitgandikota/erasing)](https://github.com/rohitgandikota/erasing) | [![arXiv](https://img.shields.io/badge/arXiv-2303.07345-b31b1b.svg)](https://arxiv.org/abs/2303.07345) | :heavy_minus_sign: |
| Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://eg3d-goae.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/jiangyzy/GOAE)](https://github.com/jiangyzy/GOAE) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12326-b31b1b.svg)](https://arxiv.org/abs/2303.12326) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CptQDMqM9Pc) |
| HairNeRF: Geometry-Aware Hair Swapped Image Synthesis | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Language

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-Training | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.11446-b31b1b.svg)](https://arxiv.org/abs/2211.11446) | :heavy_minus_sign: |
| DiffusionRet: Generative Text-Video Retrieval with Diffusion Model | [![GitHub](https://img.shields.io/github/stars/jpthu17/DiffusionRet)](https://github.com/jpthu17/DiffusionRet) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09867-b31b1b.svg)](https://arxiv.org/abs/2303.09867) | :heavy_minus_sign: |
| Explore and Tell: Embodied Visual Captioning in 3D Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aim3-ruc.github.io/ExploreAndTell/) <br /> [![GitHub](https://img.shields.io/github/stars/HAWLYQ/ET-Cap)](https://github.com/HAWLYQ/ET-Cap) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10447-b31b1b.svg)](https://arxiv.org/abs/2308.10447) | :heavy_minus_sign: |
| Distilling Large Vision-Language Model with Out-of-Distribution Generalizability | [![GitHub](https://img.shields.io/github/stars/xuanlinli17/large_vlm_distillation_ood)](https://github.com/xuanlinli17/large_vlm_distillation_ood) | [![arXiv](https://img.shields.io/badge/arXiv-2307.03135-b31b1b.svg)](https://arxiv.org/abs/2307.03135) | :heavy_minus_sign: |
| Learning Trajectory-Word Alignments for Video-Language Tasks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.01953-b31b1b.svg)](https://arxiv.org/abs/2301.01953) | :heavy_minus_sign: |
| Variational Causal Inference Network for Explanatory Visual Question Answering | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TextManiA: Enriching Visual Feature by Text-Driven Manifold Augmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://textmania.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/postech-ami/TextManiA)](https://github.com/postech-ami/TextManiA) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14611-b31b1b.svg)](https://arxiv.org/abs/2307.14611) | :heavy_minus_sign: |
| UniRef: A Unified Model for Reference-based Object Segmentation Tasks | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.06571-b31b1b.svg)](https://arxiv.org/abs/2303.06571) | :heavy_minus_sign: |
| Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pre-Training | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Toward Multi-Granularity Decision-Making: Explicit Visual Reasoning with Hierarchical Knowledge | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Moment Detection in Long Tutorial Videos | [![GitHub](https://img.shields.io/github/stars/ioanacroi/longmoment-detr)](https://github.com/ioanacroi/longmoment-detr) | :heavy_minus_sign: | :heavy_minus_sign: |
| Not All Features Matter: Enhancing Few-Shot CLIP with Adaptive Prior Refinement | [![GitHub](https://img.shields.io/github/stars/yangyangyang127/APE)](https://github.com/yangyangyang127/APE) | [![arXiv](https://img.shields.io/badge/arXiv-2304.01195-b31b1b.svg)](https://arxiv.org/abs/2304.01195) | :heavy_minus_sign: |
| Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://whoops-benchmark.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.07274-b31b1b.svg)](https://arxiv.org/abs/2303.07274) | :heavy_minus_sign: |
| Advancing Referring Expression Segmentation Beyond Single Image | [![GitHub](https://img.shields.io/github/stars/yixuan730/group-res)](https://github.com/yixuan730/group-res) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12452-b31b1b.svg)](https://arxiv.org/abs/2305.12452) | :heavy_minus_sign: |
| CLIPoint: Adapting CLIP for Powerful 3D Open-World Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Unsupervised Prompt Tuning for Text-Driven Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.09267-b31b1b.svg)](https://arxiv.org/abs/2307.09267) | :heavy_minus_sign: |
| I can't Believe there's no Images! Learning Visual Tasks using Only Language Data | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://prior.allenai.org/projects/close) <br /> [![GitHub](https://img.shields.io/github/stars/allenai/close)](https://github.com/allenai/close) | [![arXiv](https://img.shields.io/badge/arXiv-2211.09778-b31b1b.svg)](https://arxiv.org/abs/2211.09778) | :heavy_minus_sign: |
| Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples | [![GitHub](https://img.shields.io/github/stars/hengliusky/Few_shot_RVOS)](https://github.com/hengliusky/Few_shot_RVOS) | [![arXiv](https://img.shields.io/badge/arXiv-2309.02041-b31b1b.svg)](https://arxiv.org/abs/2309.02041) | :heavy_minus_sign: |
<!-- | MeViS: A Large-Scale Benchmark for Video Segmentation with Motion Expressions |  |  |  |
| Diverse Data Augmentation with Diffusions for Effective Test-Time Prompt Tuning |  |  |  |
| ShapeScaffolder: Structure-Aware 3D Shape Generation from Text |  |  |  |
| SuS-X: Training-Free Name-Only Transfer of Vision-Language Models |  |  |  |
| BEVBert: Multimodal Map Pre-Training for Language-Guided Navigation |  |  |  |
| X-Mesh: Towards Fast and Accurate Text-Driven 3D Stylization via Dynamic Textual Guidance |  |  |  |
| OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation |  |  |  |
| Attentive Mask CLIP |  |  |  |
| Knowledge Proxy Intervention for Deconfounded Video Question Answering |  |  |  |
| UniVTG: Towards Unified Video-Language Temporal Grounding |  |  |  |
| Self-Supervised Cross-View Representation Reconstruction for Change Captioning |  |  |  |
| Unified Coarse-to-Fine Alignment for Text-to-Video Retrieval |  |  |  |
| Confidence-Aware Pseudo-Label Learning for Weakly Supervised Visual Grounding |  |  |  |
| TextPSG: Panoptic Scene Graph Generation from Textual Descriptions |  |  |  |
| MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge |  |  |  |
| Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation |  |  |  |
| Transferring Visual Knowledge with Pre-Trained Models for Multimodal Machine Translation |  |  |  |
| Learning Human-Human Interactions in Images from Weak Textual Supervision |  |  |  |
| BUS:Efficient and Effective Vision-Language Pretraining with Bottom-Up Patch Summarization |  |  |  |
| 3D-VisTA: Pre-Trained Transformer for 3D Vision and Text Alignment |  |  |  |
| ALIP: Adaptive Language-Image Pre-Training with Synthetic Caption |  |  |  |
| LoGoPrompt: Synthetic Text Images can be Good Visual Prompts for Vision-Language Models |  |  |  |
| Noise-Aware Learning from Web-Crawled Image-Text Data for Image Captioning |  |  |  |
| Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering |  |  |  |
| Prompt-Guided Image Captioning for VQA with GPT-3 |  |  |  |
| Grounded Image Text Matching with Mismatched Relation Reasoning |  |  |  |
| GePSAn: Generative Procedure Step Anticipation in Cooking Videos |  |  |  |
| LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models |  |  |  |
| VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control |  |  |  |
| With a Little Help from Your own Past: Prototypical Memory Networks for Image Captioning |  |  |  |
| Improving Zero-Shot Generalization for CLIP with Synthesized Prompts |  |  |  |
| DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models |  |  |  |
| Learning Navigational Visual Representations with Semantic Map Supervision |  |  |  |
| CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection |  |  |  |
| Open Set Video HOI detection from Action-Centric Chain-of-Look Prompting |  |  |  |
| Learning Concise and Descriptive Attributes for Visual Recognition |  |  |  |
| Open-Vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models |  |  |  |
| Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories |  |  |  |
| Story Visualization by Online Text Augmentation with Context Memory |  |  |  |
| Transferable Decoding with Visual Entities for Zero-Shot Image Captioning |  |  |  |
| Too Large; Data Reduction for Vision-Language Pre-Training |  |  |  |
| ViLTA: Enhancing Vision-Language Pre-Training through Textual Augmentation |  |  |  | -->

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision, Graphics, and Robotics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Privacy, Security, Fairness, and Explainability

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Fairness, Privacy, Ethics, Social-good, Transparency, Accountability in Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### First Person (Egocentric) Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Deep Learning Architectures

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Detection

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Image and Video Synthesis

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Audio

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition, Segmentation, and Shape Analysis

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Generative AI

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Humans, 3D Modeling, and Driving

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Low-Level Vision and Theory

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Navigation and Autonomous Driving

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D from a Single Image and Shape-from-X

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Motion Estimation, Matching and Tracking

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Action and Event Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Computational Imaging

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Embodied Vision: Active Agents; Simulation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Retrieval

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Transfer, Low-Shot, Continual, Long-Tail Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Low-Level and Physics-based Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Computer Vision Theory

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Video Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Object Pose Estimation and Tracking

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D Shape Modeling and Processing

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Human Pose/Shape Estimation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Transfer, Low-Shot, and Continual Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Self-, Semi-, and Unsupervised Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Self-, Semi-, Meta-, Unsupervised Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Photogrammetry and Remote Sensing

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Efficient and Scalable Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Machine Learning (other than Deep Learning)

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Document Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Biometrics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Datasets and Evaluation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Faces and Gestures

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Medical and Biological Vision; Cell Microscopy

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Scene Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Multimodal Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Human-in-the-Loop Computer Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Image and Video Forensics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Geometric Deep Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision Applications and Systems

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Machine Learning and Dataset

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Unmasked Teacher: Towards Training-Efficient Video Foundation Models | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/unmasked_teacher)](https://github.com/OpenGVLab/unmasked_teacher) | [![arXiv](https://img.shields.io/badge/arXiv-2303.16058-b31b1b.svg)](https://arxiv.org/abs/2303.16058) | :heavy_minus_sign: |

---

## Star History

<p align="center">
    <a href="https://star-history.com/#Dmitryryumin/ICCV-2023-Papers&Date" target="_blank">
        <img width="500" src="https://api.star-history.com/svg?repos=Dmitryryumin/ICCV-2023-Papers&type=Date" alt="Star History Chart">
    </a>
<p>
